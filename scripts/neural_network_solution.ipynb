{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution with Neural Network only, which gives 0.547 on private leaderboard (Silver medal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Блокнот, подводящий промежуточные итоги по подготовке данных: чистка, извлечение признаков, создание новых признаков. Предполагается, что этот блокнот будет в дальнейшем использоваться как база для исследования и сравнения различных алгоритмов машинного обучения, генерации мета-признаков и пр.\n",
    "\n",
    "В основе заготовки - популярное решение по обработке лога от Hosseinali (и немного от Andrew Lukyanenko). К нему добавлено:\n",
    "\n",
    "- чистка данных (удаление сессий с малым числом событий, сокращение сессий с перерывами и пр.)\n",
    "- новые признаки (свойства по различным топикам, временным промежуткам, успешность в играх и пр.)\n",
    "- решена проблема с невоспроизводимостью предварительной обработки данных\n",
    "- включает решение 557 (https://www.kaggle.com/panikads/data-processing-consolidated-376397 - версия 2)\n",
    "- включает результат рекурсивного удаления признаков (https://www.kaggle.com/ponomarevav/547-rfecv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, cv, Pool\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = 'data'   # Home\n",
    "#DATA_DIR = 'E:/Ponomarev/dsbowl2019/data'  # Work\n",
    "DATA_DIR = '../input/data-science-bowl-2019' # Kaggle\n",
    "\n",
    "# Разные настройки\n",
    "######################################################################\n",
    "\n",
    "RANDOM_SEED = 17\n",
    "\n",
    "\n",
    "### Разное\n",
    "\n",
    "# Сохранять обработанные наборы данных с помощью pickle\n",
    "# (например, чтобы их можно было читать из других кернелов и экономить\n",
    "# время на обработку)\n",
    "SAVE_PROCESSED_DATASETS = True\n",
    "\n",
    "# Сделать adversarial validation\n",
    "ADVERSARIAL_VALIDATION = True\n",
    "\n",
    "# Сохранить результаты классификации в файл (для последующего анализа ошибок)\n",
    "# Увеличивает время работы ноутбука (примерно на время кросс-валидации)\n",
    "SAVE_PREDICTED_TARGETS = True\n",
    "\n",
    "# Блединг моделей с разным начальным состоянием генератора случайных чисел\n",
    "# ВНИМАНИЕ! Сильно замедляет работу скрипта. Включать только для сабмита. \n",
    "# ВНИМАНИЕ! Неправильно работает в сочетании с параноидальной CV.\n",
    "MULTISEED_BLEND = False\n",
    "MULTISEED_BLEND_MODEL_COUNT = 5\n",
    "\n",
    "\n",
    "### Предварительная обработка\n",
    "\n",
    "# Сортировать события (устраняет ошибки в порядке событий - их мало, но они есть)\n",
    "FORCE_CORRECT_EVENT_ORDER = True\n",
    "\n",
    "# Стоит ли удалять признаки, имеющие высокую корреляцию с другими\n",
    "DROP_HIGHLY_CORRELATED = False\n",
    "\n",
    "# Стоит ли удалять признаки, которые были признаны не значимыми\n",
    "# в ходе процедуры рекурсивного удаления\n",
    "DROP_UNIMPORTANT = False\n",
    "\n",
    "\n",
    "### Кросс-валидация\n",
    "\n",
    "# Количество фолдов при \"обычной\" оценке\n",
    "ORDINARY_CV = False\n",
    "ORDINARY_CV_FOLD_COUNT = 5\n",
    "\n",
    "# Кросс-валидация на разных моделях с (относительно) робастной оценкой\n",
    "PARANOIDAL_CV = False\n",
    "PARANOIDAL_CV_SEED_COUNT = 2\n",
    "PARANOIDAL_CV_FOLD_COUNT = 3\n",
    "\n",
    "# Кросс-валидация с одним тестом на пользователя\n",
    "ONE_PER_USER_CV = False\n",
    "ONE_PER_USER_CV_FOLD_COUNT = 5\n",
    "\n",
    "\n",
    "### Использовать ли псевдолейблинг:\n",
    "USE_PSEUDOLABELING = False\n",
    "# True - используем вещественные целевые переменные, False - классы для теста при псевдолейблинге.\n",
    "USE_SOFTLABELS = True\n",
    "\n",
    "# True - использовать ли GPU для обучения (в кернеле всегда должно быть False!)\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 10.9 s, total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), parse_dates=['timestamp'])\n",
    "train_labels = pd.read_csv(os.path.join(DATA_DIR, 'train_labels.csv'))\n",
    "test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), parse_dates=['timestamp'])\n",
    "                   \n",
    "#specs = pd.read_csv(os.path.join(DATA_DIR, 'specs.csv')\n",
    "#sample_submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
    "\n",
    "# Флаг, позволяющий отключать некоторые действия для ускорения сабмита.\n",
    "# Инициализация флага основывается на том факте, что публичный тестовый набор\n",
    "# содержит около 1 млн. записей, а приватный (на котором кернел работает в\n",
    "# случае сабмита) - около 8 млн.\n",
    "IS_SUBMITTING = test.shape[0] > 4000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_train_size = 2 * len(train) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = train[debug_train_size:].copy()\n",
    "# test_labels = train_labels[debug_train_size:].copy()\n",
    "# train = train[:debug_train_size].copy()\n",
    "# train_labels = train_labels[:debug_train_size].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уберем из обучающего набора те данные, которых нет в train_labels\n",
    "\n",
    "*Замечание!* Это действие складывается из двух - удаление логов пользователей, у которых вообще нет assessment, и удаление логов пользователей, для которых нет метки в `train_labels`. Первое подмножество пользователей, очевидно, входит во второе (или совпадает с ним), поэтому удаление пользователей без assessment можно не производить, если оставляем только те, которые есть в `train_labels`. Другое дело, что обучающем наборе есть такие сессии assessment, которых почему-то нет в `train_labels` (например, b2f47fa73899b10d, 35af4f36098afcf4 и некоторые другие - `train[(~train.installation_id.isin(train_labels.installation_id.unique())) & (train.type == 'Assessment')]`), но мы можем их генерировать сами - тогда, возможно, стоит удалять логи пользователей без assessment, но отключить второй блок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# При наличии следующей ячейки это и не нужно!\n",
    "# \n",
    "## Выкинем из логов обучающего набора логи тех пользователей, у которых вообще нет\n",
    "## assessment.\n",
    "## Возможно, в дальшейшем я придумаю как их можно использовать, но пока - удаляем.\n",
    "#keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n",
    "#train = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7734558, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выкинем из обучающего набора логи тех пользователей, которых нет \n",
    "# в train_labels\n",
    "train = train[train.installation_id.isin(train_labels.installation_id.unique())]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_labels: 17690\n",
      "Number of unique game_sessions in train_labels: 17690\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что размер обучающего лога совпадает с имеющейся разметкой\n",
    "print(f'Number of rows in train_labels: {train_labels.shape[0]}')\n",
    "print(f'Number of unique game_sessions in train_labels: {train_labels.game_session.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>game_session</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>event_data</th>\n",
       "      <th>installation_id</th>\n",
       "      <th>event_count</th>\n",
       "      <th>event_code</th>\n",
       "      <th>game_time</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>27253bdc</td>\n",
       "      <td>34ba1a28d02ba8ba</td>\n",
       "      <td>2019-08-06 04:57:18.904000+00:00</td>\n",
       "      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to Lost Lagoon!</td>\n",
       "      <td>Clip</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>27253bdc</td>\n",
       "      <td>4b57c9a59474a1b9</td>\n",
       "      <td>2019-08-06 04:57:45.301000+00:00</td>\n",
       "      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Magma Peak - Level 1</td>\n",
       "      <td>Clip</td>\n",
       "      <td>MAGMAPEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>77261ab5</td>\n",
       "      <td>2b9d5af79bcdb79f</td>\n",
       "      <td>2019-08-06 04:58:14.538000+00:00</td>\n",
       "      <td>{\"version\":\"1.0\",\"event_count\":1,\"game_time\":0...</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Sandcastle Builder (Activity)</td>\n",
       "      <td>Activity</td>\n",
       "      <td>MAGMAPEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>b2dba42b</td>\n",
       "      <td>2b9d5af79bcdb79f</td>\n",
       "      <td>2019-08-06 04:58:14.615000+00:00</td>\n",
       "      <td>{\"description\":\"Let's build a sandcastle! Firs...</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>2</td>\n",
       "      <td>3010</td>\n",
       "      <td>29</td>\n",
       "      <td>Sandcastle Builder (Activity)</td>\n",
       "      <td>Activity</td>\n",
       "      <td>MAGMAPEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>1325467d</td>\n",
       "      <td>2b9d5af79bcdb79f</td>\n",
       "      <td>2019-08-06 04:58:16.680000+00:00</td>\n",
       "      <td>{\"coordinates\":{\"x\":273,\"y\":650,\"stage_width\":...</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>3</td>\n",
       "      <td>4070</td>\n",
       "      <td>2137</td>\n",
       "      <td>Sandcastle Builder (Activity)</td>\n",
       "      <td>Activity</td>\n",
       "      <td>MAGMAPEAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      event_id      game_session                        timestamp  \\\n",
       "1538  27253bdc  34ba1a28d02ba8ba 2019-08-06 04:57:18.904000+00:00   \n",
       "1539  27253bdc  4b57c9a59474a1b9 2019-08-06 04:57:45.301000+00:00   \n",
       "1540  77261ab5  2b9d5af79bcdb79f 2019-08-06 04:58:14.538000+00:00   \n",
       "1541  b2dba42b  2b9d5af79bcdb79f 2019-08-06 04:58:14.615000+00:00   \n",
       "1542  1325467d  2b9d5af79bcdb79f 2019-08-06 04:58:16.680000+00:00   \n",
       "\n",
       "                                             event_data installation_id  \\\n",
       "1538             {\"event_code\": 2000, \"event_count\": 1}        0006a69f   \n",
       "1539             {\"event_code\": 2000, \"event_count\": 1}        0006a69f   \n",
       "1540  {\"version\":\"1.0\",\"event_count\":1,\"game_time\":0...        0006a69f   \n",
       "1541  {\"description\":\"Let's build a sandcastle! Firs...        0006a69f   \n",
       "1542  {\"coordinates\":{\"x\":273,\"y\":650,\"stage_width\":...        0006a69f   \n",
       "\n",
       "      event_count  event_code  game_time                          title  \\\n",
       "1538            1        2000          0        Welcome to Lost Lagoon!   \n",
       "1539            1        2000          0           Magma Peak - Level 1   \n",
       "1540            1        2000          0  Sandcastle Builder (Activity)   \n",
       "1541            2        3010         29  Sandcastle Builder (Activity)   \n",
       "1542            3        4070       2137  Sandcastle Builder (Activity)   \n",
       "\n",
       "          type      world  \n",
       "1538      Clip       NONE  \n",
       "1539      Clip  MAGMAPEAK  \n",
       "1540  Activity  MAGMAPEAK  \n",
       "1541  Activity  MAGMAPEAK  \n",
       "1542  Activity  MAGMAPEAK  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing correct event order. Before (train):\n",
      "Time delta errors: 13\n",
      "Event order errors: 122073\n",
      "Doing sort...\n",
      "After (train):\n",
      "Time delta errors: 0\n",
      "Event order errors: 0\n"
     ]
    }
   ],
   "source": [
    "def report_irregularities(df):\n",
    "    time_delta = df.game_time - df.game_time.shift(1, fill_value=0)\n",
    "    count_delta = df.event_count - df.event_count.shift(1, fill_value=0)\n",
    "    print('Time delta errors:', ((time_delta  < 0) & (df.game_session == df.game_session.shift(1))).sum())\n",
    "    print('Event order errors:', ((count_delta  < 0) & (df.game_session == df.game_session.shift(1))).sum())    \n",
    "\n",
    "if FORCE_CORRECT_EVENT_ORDER:\n",
    "    \n",
    "    def fix_order(df):\n",
    "        # Разметим сессии временем начала, чтобы они были упорядочены хронологически в рамках\n",
    "        # истории пользователя\n",
    "        df['session_start'] = df.groupby(['game_session'])['timestamp'].transform('min')\n",
    "        # Обеспечим желаемый порядок\n",
    "        df.sort_values(['installation_id', 'session_start', 'game_session', 'event_count'], inplace=True)\n",
    "        df.drop(columns=['session_start'], inplace=True)    \n",
    "        \n",
    "    print('Forcing correct event order. Before (train):')\n",
    "    report_irregularities(train)\n",
    "    print('Doing sort...')\n",
    "    fix_order(train)\n",
    "    fix_order(test)\n",
    "    print('After (train):')\n",
    "    \n",
    "report_irregularities(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение проблемы с длинными сессиями\n",
    "\n",
    "Потенциальных решений несколько:\n",
    "\n",
    "- удаление аномально длинных сессий\n",
    "- корректировка game_time (для сессий с перерывами) - перерыв просто \"вырезается\" из записи (эксперименты Татьяны показали, что это не вполне себя оправдывает. Код присутствует (но не работает) в более ранних версиях кернела.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Просто убираем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHAT_IS_LONG = 1000 * 60 * 60 * 2 # Порог (мс), после которого сессия считается \"аномально длинной\"\n",
    "\n",
    "long_sessions = train[train['game_time'] > WHAT_IS_LONG]['game_session'].unique()\n",
    "# Оставляем только те записи, которые относятся не длинным сессиям или являются тестированиями\n",
    "train = train[(~(train.game_session.isin(long_sessions))) | (train.type == 'Assessment')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Типы событий, соответствующие активным действиям пользователя\n",
    "USER_ACTION_EVENT_CODES = [\n",
    "    4010,   # Кликает для начала игры\n",
    "    4020,   # Что-то куда-то тянет, перетаскивает и пр.\n",
    "    4021,   # Что-то куда-то кладет\n",
    "    4022,   # Что-то куда-то кладет\n",
    "    4025,   # Конец перетаскивания\n",
    "    4030,   # Кликает на игровой объект или начинает перетаскивать\n",
    "    4031,  \n",
    "    4035,   # Перетаскивает что-то куда-то не туда (!!!)\n",
    "    4040,   # Что-то куда-то тащит\n",
    "    4045,\n",
    "    4050,\n",
    "    4070,   # Misclick (!!!)\n",
    "    4080,   # Наводит мышкой на интерактивный объект\n",
    "    4090,   # Запрос помощи\n",
    "    4095,   # Play again\n",
    "    4100,   # Submit a solution\n",
    "    4110,   # Submit a solution\n",
    "    4220,   # Кликает на интерактивном объекте на экране победы\n",
    "    4230,   # Начал тащить bath toy\n",
    "    4235,   # Закончил тащить bath toy\n",
    "]\n",
    "\n",
    "# пара вспомогательных признаков по событиям\n",
    "train['user_action'] = train.event_code.isin(USER_ACTION_EVENT_CODES)\n",
    "train['delay'] = train.game_time - train.game_time.shift(1, fill_value=0)\n",
    "test['user_action'] = test.event_code.isin(USER_ACTION_EVENT_CODES)\n",
    "test['delay'] = test.game_time - test.game_time.shift(1, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "В этой секции осуществляется переход от логов к обычным табличным данным, в которых каждая запись содержит все признаки, на основе которых будем пытаться предсказывать результат.\n",
    "\n",
    "Сам код взят из https://www.kaggle.com/erikbruin/data-science-bowl-2019-data-exploration, который, в свою очередь, позаимствовал его из:\n",
    "\n",
    "- Massoud Hosseinali: https://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model\n",
    "- Andrew Lukyanenko\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits go to Andrew Lukyanenko\n",
    "\n",
    "def encode_strings(train, test, train_labels):\n",
    "    \"\"\"\n",
    "    Функция осуществляет кодирование строковых (title, world) и некоторых категорийных \n",
    "    (event_id, event_code) в числовые в диапазоне от 0 до количества уникальных значений.\n",
    "    \n",
    "    Возвращает модифицированные наборы и пары словарей для преобразования старого значения\n",
    "        в новое и обратно.\n",
    "    \"\"\"\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    # ПАВ: all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n",
    "    list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, \\\n",
    "           list_of_event_code, activities_labels, assess_titles, list_of_event_id\n",
    "\n",
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, \\\n",
    "    list_of_event_code, activities_labels, assess_titles, list_of_event_id = encode_strings(train, test, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение видов заданий по тематическим частям \"мира\" (поле world)\n",
    "world_titles = {}\n",
    "for world in train.world.unique():\n",
    "    world_titles[world] = list(train.title[train.world == world].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_titles_num = train[train['type']=='Game']['title'].unique()\n",
    "games_titles = []\n",
    "for i in games_titles_num:\n",
    "    games_titles.append(activities_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_game_code = {'Scrub-A-Dub' : 4020, \n",
    "                 'Chow Time': 4020, \n",
    "                 'All Star Sorting' : 4020, \n",
    "                 'Air Show' : 4100, \n",
    "                 'Crystals Rule' : 4020, \n",
    "                 'Dino Drink' : 4020, \n",
    "                 'Bubble Bath' : 4020, \n",
    "                 'Dino Dive' : 4020, \n",
    "                 'Pan Balance' : 4100, \n",
    "                 'Happy Camel' : 4020, \n",
    "                 'Leaf Leader' : 4020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import bisect\n",
    "\n",
    "# Специальный флаг для пометки значений, получающихся при делении на 0.\n",
    "# Идея в том, что это значение должно находиться вне диапазона \"нормальных\"\n",
    "# значений признака. Можно использовать np.nan и потом \"чистить\" пропуски.\n",
    "DIV_BY_ZERO = 0\n",
    "\n",
    "# Credits go to Massoud Hosseinali\n",
    "\n",
    "# Эта функция осуществляет всю \"магию\" преобразования из лога (журнала) данных по одному\n",
    "# пользователю в таблицу признаков.\n",
    "\n",
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only required\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # News features: time spent in each activity\n",
    "    time_spent_each_act = {actv: 0 for actv in list_of_user_activities} # для каждого title\n",
    "    event_code_count = {eve: 0 for eve in list_of_event_code}           # для каждого event_code\n",
    "    \n",
    "    accumulated_game_accuracy = {game: [] for game in games_titles}\n",
    "\n",
    "    #активных действий на раунд (за первые 3 раунда)\n",
    "    action_per_round = {game: [] for game in games_titles}\n",
    "    \n",
    "    #переменные для фиксации промежуточных результатов по каждому тесту\n",
    "    assess_interim_accuracy = {assess: [] for assess in assess_titles}\n",
    "    \n",
    "    # Тематические признаки (по world)\n",
    "    features_by_world_template = {\n",
    "        'time_spent': 0,        # время, проведенное в мире (сек)\n",
    "        'actions_count': 0,     # количество действий в мире\n",
    "        'session_types': {typ: 0 for typ in ['Clip', 'Activity', 'Assessment', 'Game']}, # количество сессий разных типов\n",
    "        'correct_assessment_attempts': 0,    # количество правильных ответов (в тестах)\n",
    "        'incorrect_assessment_attempts': 0,  # количество неправильных ответов (в тестах)\n",
    "        'accumulated_interim_accuracy' : [], # точность по промежуточным действиям в тесте\n",
    "        'game_accuracy' : [] ,               # точность в играх\n",
    "    }\n",
    "    features_by_world = {wrld: copy.deepcopy(features_by_world_template) \n",
    "                             for wrld in world_titles.keys()}\n",
    "    \n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    # Аккумулятор, в котором накапливаются записи по данному пользователю\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = []\n",
    "    # Средняя точность по всем сессиям assessment\n",
    "    accumulated_accuracy = 0\n",
    "    \n",
    "    #Точность по промежуточным действиям тестов\n",
    "    accumulated_interim_accuracy = []\n",
    "    \n",
    "    # Общее количество правильных ответов во всех сессиях assessment\n",
    "    accumulated_correct_attempts = 0 \n",
    "    # Общее количество неправильных ответов во всех сессиях assessment\n",
    "    accumulated_uncorrect_attempts = 0 \n",
    "    # Общее количество действий, совершенных пользователем (по факту - вообще событий)\n",
    "    accumulated_actions = 0\n",
    "    # Количество обработанных сессий (типа assessment)\n",
    "    assessment_count = 0\n",
    "    # Количество законченных сессий типа assessment\n",
    "    completed_assessment_count = 0\n",
    "    # Количество обработанных сессий (любого типа)\n",
    "    session_count = 0\n",
    "    # Время первого события для текущего пользователя\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    # Список, в который помещаются продолжительности сессий, чтобы посчитать среднюю\n",
    "    durations = []\n",
    "    # Точность, показанная при предыдущем выполнении теста (assessment)\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}    \n",
    "    # Счетчики различных характеристик событий в истории пользователя:\n",
    "    # кодов событий, идентификаторов, собственно заданий\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    # title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code} # ПАВ: чересчур!\n",
    "    \n",
    "    # Время окончания последней сессии (используется для определения длительных перерывов между сессиями\n",
    "    # и длительных \"кусков\" непрерывной игры)\n",
    "    last_session_finished = user_sample['timestamp'].iloc[0]\n",
    "    # Начало последнего \"непрерывного блока\" игры (без отдыха хотя бы в 1 час)\n",
    "    continuous_block_start = last_session_finished\n",
    "\n",
    "    # Список показателей с временными метками (для определения значений\n",
    "    # признаков за определенный интервал)\n",
    "    # Самое простое решение - это так выбрать содержимое этих признаков, чтобы они точно соответствовали\n",
    "    # каким-то переменным-аккумуляторам, используемым в этой функции\n",
    "    # TODO: Это можно обобщить еще \"хитрее\", просто перечислив набор выражений, из которого\n",
    "    #    должны формироваться такие временные срезы, а потом запускать что-то вроде eval() для каждого из\n",
    "    #    этих выражений!\n",
    "    timed_data_template = {\n",
    "        '4070': event_code_count[4070],                                 # Количество событий 4070\n",
    "        'correct_assessment_attempts': accumulated_correct_attempts,    # Количество правильных ответов на задания теста\n",
    "        'incorrect_assessment_attempts': accumulated_uncorrect_attempts,# Количество неправильных ответов на задания теста\n",
    "        'accumulated_assessment_accuracy': accumulated_accuracy,        # Аккумулированное значение точности (для вычисления средней)\n",
    "        'assessment_count': assessment_count,                           # Количество сессий типа Assessment\n",
    "        'actions': accumulated_actions,                                 # Количество действий\n",
    "        'accumulated_session_duration': np.sum(durations),              # Общая продолжительность сессий\n",
    "        'session_count': session_count,                                 # Количество сессий\n",
    "    }\n",
    "    # Принцип хранения данных с временными метками следующий:\n",
    "    #   в timed_data_values хранятся агрегированные значения показателей с начала истории пользователя\n",
    "    #       до окончания некоторой сессии\n",
    "    #   в timed_data_time хранятся временные метки (соответствующие моментам окончания сессий)\n",
    "    # Длины списков совпадают. `timed_data_time` используется для поиска нужного среза. \n",
    "    timed_data_time = [user_sample.timestamp.iloc[0] - datetime.timedelta(days=300)]\n",
    "    timed_data_values = [copy.deepcopy(timed_data_template)]    \n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title] # from Andrew Lukyanenko\n",
    "        session_start = session['timestamp'].iloc[0]\n",
    "        session_world = session['world'].iloc[0]\n",
    "        \n",
    "        # отбрасываем сессии, где меньше 6 событий\n",
    "        # TODO: перенести в предварительную обработку?\n",
    "\n",
    "        # if ((session_type == 'Clip') or (len(session)>5) or ((test_set) & (session_type == 'Assessment'))):\n",
    "        if (session_type != 'Clip') and (len(session)<6) and (not test_set or (session_type != 'Assessment')):\n",
    "            continue\n",
    "\n",
    "        # Время начала последнего \"непрерывного\" блока действий, предшествующего \n",
    "        # началу данной сессии (может быть связано с утомленностью пользователя)\n",
    "        # Под \"непрерывным блоком\" понимаем отдых меньше часа\n",
    "        if (session_start - last_session_finished).seconds > 60*60:\n",
    "            continuous_block_start = session_start\n",
    "\n",
    "        # get current session time in seconds\n",
    "        # Странно, что проведенное время считается только не в Assessment...\n",
    "        # может, это поправить???\n",
    "        if session_type != 'Assessment':\n",
    "            time_spent = int(session['game_time'].iloc[-1] / 1000)\n",
    "            time_spent_each_act[activities_labels[session_title]] += time_spent\n",
    "            features_by_world[session_world]['time_spent'] += time_spent\n",
    "\n",
    "            if session_type == 'Game':\n",
    "                all_game_attempts = session.query(f'event_code == {win_game_code[session_title_text]}')\n",
    "                game_true_attempts = all_game_attempts['event_data'].str.contains('true').sum()\n",
    "                game_false_attempts = all_game_attempts['event_data'].str.contains('false').sum()\n",
    "                if (game_true_attempts+game_false_attempts) > 0:\n",
    "                    game_accuracy = game_true_attempts/(game_true_attempts+game_false_attempts) \n",
    "                    accumulated_game_accuracy[session_title_text].append(game_accuracy)\n",
    "                    features_by_world[session_world]['game_accuracy'].append(game_accuracy)\n",
    "                all_2030 = session[session['event_code'] == 2030].index\n",
    "                if len (all_2030) > 0:\n",
    "                    if len (all_2030) > 3:\n",
    "                        last_2030 = all_2030[2]\n",
    "                        num_rounds = 3\n",
    "                    else:\n",
    "                        last_2030 = all_2030[-1]\n",
    "                        num_rounds = len (all_2030)\n",
    "                    act_action = len(session.loc[:last_2030][session.loc[:last_2030]['user_action'] == True])\n",
    "                    action_per_round[session_title_text].append(act_action/num_rounds)\n",
    "                    \n",
    "        # for each assessment, and only this kind of session, the features below are processed\n",
    "        # and a register is generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            \n",
    "            #выбираем все правильные и неправильные действия вне финального Done\n",
    "            all_interim_attempts = session.query(f'event_code != {win_code[session_title]}')\n",
    "            true_interim_attempts = all_interim_attempts['event_data'].str.contains('true').sum()\n",
    "            false_interim_attempts = all_interim_attempts['event_data'].str.contains('false').sum()  \n",
    "            \n",
    "            # copy a dict to use as feature template, it's initialized with some items: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())    # 'acc_XXX'\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            # features.update(time_spent_each_act.copy())  # убрал вслед за Aquino...\n",
    "            \n",
    "            all_game_action_per_round = []\n",
    "            for game in games_titles:\n",
    "                if len(accumulated_game_accuracy[game])>0:\n",
    "                    features[f'game_{game}_accuracy_last'] = accumulated_game_accuracy[game][-1]            \n",
    "                    features[f'game_{game}_accuracy_mean'] = np.mean(accumulated_game_accuracy[game])\n",
    "                if len(action_per_round[game])>0:\n",
    "                    features[f'game_{game}_action_per_round'] = action_per_round[game][-1]\n",
    "                    all_game_action_per_round.append(action_per_round[game][-1])\n",
    "            features['all_game_action_per_round'] = np.mean(all_game_action_per_round)\n",
    "           \n",
    "            # Признаки, характеризующие \"разнообразие\" опыта пользователя, 'var_XXX'\n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                                ('var_event_id', event_id_count),\n",
    "                                ('var_title', title_count),\n",
    "                               # ('var_title_event_code', title_event_code_count) # ПАВ: чересчур!\n",
    "                               ]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)            \n",
    "            \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1] # from Andrew L.\n",
    "            # timestamp of session start (for timed features)\n",
    "            features['timestamp'] = session.timestamp.iloc[0]\n",
    "            # тема (топик, world) задания\n",
    "            features['world'] = session.world.iloc[0]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0] \n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            # количество правильных и неправильных попыток в данной теме (world)\n",
    "            for world in world_titles.keys():\n",
    "                features[f'world_{world}_accumulated_correct'] = features_by_world[world]['correct_assessment_attempts']\n",
    "                features[f'world_{world}_accumulated_incorrect'] = features_by_world[world]['incorrect_assessment_attempts']\n",
    "                features[f'world_{world}_mean_game_accuracy'] = np.mean(features_by_world[world]['game_accuracy'])\n",
    "\n",
    "            # количество правильных и неправильных попыток за разные промежутки времени\n",
    "            for prev_days in [3, 7, 14]:\n",
    "                dt = session_start - datetime.timedelta(days=prev_days)\n",
    "                prev = timed_data_values[bisect.bisect_left(timed_data_time, dt)-1]\n",
    "                cur = timed_data_values[-1]\n",
    "                # считаем, что все признаки, лежащие в timed_features нужно формировать для каждого \n",
    "                # временного интервала. Потом это можно настроить.\n",
    "                for feature in timed_data_template.keys():\n",
    "                    features['%ddays_%s' % (prev_days, feature)] = cur[feature] - prev[feature]\n",
    "                \n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # запомним также статистику правильности ответов в данной теме\n",
    "            features_by_world[session_world]['correct_assessment_attempts'] += true_attempts\n",
    "            features_by_world[session_world]['incorrect_assessment_attempts'] += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = DIV_BY_ZERO\n",
    "                features['duration_std'] = DIV_BY_ZERO\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            # the accuracy is the all time wins divided by the all time attempts\n",
    "            features['mean_accuracy2'] = accumulated_accuracy/assessment_count if assessment_count > 0 else DIV_BY_ZERO\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy            \n",
    "            ## Целевые признаки (не забывать удалять их все перед обучением!)\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features['accuracy'] = accuracy\n",
    "            features['num_correct'] = true_attempts\n",
    "            features['num_incorrect'] = false_attempts\n",
    "            ##\n",
    "\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            \n",
    "            features['current_assess_interim_accuracy'] = np.mean(assess_interim_accuracy[session_title_text])\n",
    "            for world in world_titles.keys():\n",
    "                features[f'world_{world}_accumulated_interim_accuracy'] = np.mean(features_by_world[world]['accumulated_interim_accuracy'])            \n",
    "            features['accumulated_interim_accuracy'] = np.mean(accumulated_interim_accuracy)\n",
    "            if (true_interim_attempts+false_interim_attempts) >0:\n",
    "                interim_accuracy = true_interim_attempts/(true_interim_attempts+false_interim_attempts) \n",
    "                accumulated_interim_accuracy.append(interim_accuracy)\n",
    "                features_by_world[session_world]['accumulated_interim_accuracy'].append(interim_accuracy)\n",
    "                assess_interim_accuracy[session_title_text].append(interim_accuracy)\n",
    "            \n",
    "            # mean of the all accuracy groups of this player\n",
    "            if len(accumulated_accuracy_group) > 0:\n",
    "                features['accumulated_accuracy_group'] = np.mean(accumulated_accuracy_group)\n",
    "            accumulated_accuracy_group.append(features['accuracy_group'])\n",
    "\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            # непрерывное время взаимодействия с приложением к моменту начала сессии (секунды)\n",
    "            features['continuous_block'] = (session_start - continuous_block_start).seconds\n",
    "            # количество действий к данному моменту в каждой из тем (world)\n",
    "            for world in world_titles.keys():\n",
    "                features['world_%d_action_count' % (world,)] = features_by_world[world]['actions_count']\n",
    "            # количество сессий разных типов в каждой из тем (world)\n",
    "            for world in world_titles.keys():\n",
    "                for typ in ['Clip', 'Activity', 'Assessment', 'Game']:\n",
    "                    features['world_%d_%s_count' % (world, typ)] = features_by_world[world]['session_types'][typ]\n",
    "\n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                completed_assessment_count += 1\n",
    "\n",
    "            assessment_count += 1\n",
    "\n",
    "        # С помощью этой функции и ее вызовов (которые следуют непосредственно за ней)\n",
    "        # подсчитываются количества разных видов событий в сессии и истории пользователя\n",
    "        def update_counters(counter: dict, col: str):\n",
    "            num_of_session_count = Counter(session[col])\n",
    "            for k in num_of_session_count.keys():\n",
    "                x = k\n",
    "                if col == 'title':\n",
    "                    x = activities_labels[k]\n",
    "                counter[x] += num_of_session_count[k]\n",
    "            return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        # title_event_code_count = update_counters(title_event_code_count, 'title_event_code') # ПАВ: чересчур!\n",
    "            \n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        user_activities_count[session_type] += 1\n",
    "        features_by_world[session_world]['actions_count'] += len(session)\n",
    "        features_by_world[session_world]['session_types'][session_type] += 1\n",
    "\n",
    "        # отметим время завершения сессии\n",
    "        last_session_finished = session.timestamp.iloc[-1]\n",
    "\n",
    "        # общее количество обработанных сессий\n",
    "        session_count += 1\n",
    "        \n",
    "        # занесем агрегированные данные в наш аккумулированный временной ряд\n",
    "        timed_data_time.append(last_session_finished)\n",
    "        timed_data_values.append({\n",
    "            '4070': event_code_count[4070],                                 # Количество событий 4070\n",
    "            'correct_assessment_attempts': accumulated_correct_attempts,    # Количество правильных ответов на задания теста\n",
    "            'incorrect_assessment_attempts': accumulated_uncorrect_attempts,# Количество неправильных ответов на задания теста\n",
    "            'accumulated_assessment_accuracy': accumulated_accuracy,        # Аккумулированное значение точности (для вычисления средней)\n",
    "            'assessment_count': assessment_count,                           # Количество (завершенных) сессий типа Assessment\n",
    "            'actions': accumulated_actions,                                 # Количество действий\n",
    "            'accumulated_session_duration': np.sum(durations),              # Общая продолжительность сессий\n",
    "            'session_count': session_count,                                 # Количество сессий\n",
    "        })        \n",
    "        \n",
    "    # if test_set=True, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in train_set, all assessments are kept\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c4ff929aa14dd4a25a2fde0fa53b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Installation_id', max=3614.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Credits go to Massoud Hosseinali\n",
    "\n",
    "# The get_data function is applied to each installation_id and added to the compile_data list\n",
    "compiled_data = []\n",
    "# tqdm is the library that draws the status bar below\n",
    "for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "    # user_sample is a DataFrame that contains only one installation_id\n",
    "    compiled_data += get_data(user_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17687, 593)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Credits go to Massoud Hosseinali\n",
    "\n",
    "# Compiled_data is converted into a DataFrame and deleted to save memory\n",
    "train_wide = pd.DataFrame(compiled_data)\n",
    "del compiled_data\n",
    "train_wide.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание дополнительных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(train_wide):\n",
    "    # Признак \"поздно\"\n",
    "    train_wide['late'] = ((train_wide.timestamp.dt.hour >= 21) | (train_wide.timestamp.dt.hour < 4)).astype('int')\n",
    "    \n",
    "    # \"Продолжительность игры в определенном разделе мира\"\n",
    "    for world in world_titles.keys():\n",
    "        activities_titles = map(lambda x: activities_labels[x], world_titles[world])\n",
    "        train_wide['world_%d_total_time' % (world,)] = train_wide[activities_titles].sum(axis=1)    \n",
    "        \n",
    "    # Интегральная точность по каждому из топиков\n",
    "    for world in world_titles.keys():\n",
    "        # Это способ расчета по общему количеству ответов\n",
    "        train_wide[f'world_{world}_accuracy1'] = train_wide[f'world_{world}_accumulated_correct'] / (train_wide[f'world_{world}_accumulated_correct'] + train_wide[f'world_{world}_accumulated_incorrect'] + 0.1)\n",
    "        # TODO: Это способ расчета по точности на каждом тесте (как в process())\n",
    "        \n",
    "    # Статистика и успеваемость по текущему топику\n",
    "    # TODO: отрефакторить!!! (select?)\n",
    "    for feature_suffix in ['total_time', \n",
    "                           'accumulated_correct', \n",
    "                           'accumulated_incorrect', \n",
    "                           'accuracy1',\n",
    "                           'accumulated_interim_accuracy',\n",
    "                           'mean_game_accuracy'\n",
    "                          ]:\n",
    "        train_wide[f'current_world_{feature_suffix}'] = 0\n",
    "        for world in world_titles.keys():\n",
    "            train_wide.loc[(train_wide.world == world), [f'current_world_{feature_suffix}']] = train_wide[f'world_{world}_{feature_suffix}'][train_wide.world == world]\n",
    "    for typ in ['Clip', 'Activity', 'Assessment', 'Game']:\n",
    "        train_wide['current_world_%s_count' % (typ,)] = 0\n",
    "        for world in world_titles.keys():\n",
    "            train_wide.loc[(train_wide.world == world), ['current_world_%s_count' % (typ,)]] = train_wide['world_%d_%s_count' % (world, typ)][train_wide.world == world]\n",
    "\n",
    "    # Нормализация временных признаков\n",
    "    for prev_days in [3, 7, 14]:\n",
    "        train_wide[f'{prev_days}days_assessment_accuracy_mean'] = train_wide[f'{prev_days}days_accumulated_assessment_accuracy'] / (train_wide[f'{prev_days}days_assessment_count'] + 0.01)\n",
    "        train_wide[f'{prev_days}days_4070_ratio'] = train_wide[f'{prev_days}days_4070'] / (train_wide[f'{prev_days}days_actions'] + 0.01)\n",
    "            \n",
    "    # Для обеспечения воспроизводимости результатов, признаки необходимо упорядочить\n",
    "    # (заодно перейдем к исключительно строковым названиям)\n",
    "\n",
    "    # Сначала перейдем к колонкам-строкам (иначе можем \"споткнуться\" о невозможность сравнения)\n",
    "    train_wide.rename(columns=str, inplace=True)\n",
    "    # Теперь просто отсортируем колонки\n",
    "    train_wide = train_wide[sorted(train_wide.columns.to_list())]    \n",
    "    \n",
    "    return train_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем все дополнительные признаки\n",
    "train_wide = make_features(train_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0086365d</th>\n",
       "      <th>00c73085</th>\n",
       "      <th>01ca3a3c</th>\n",
       "      <th>022b4259</th>\n",
       "      <th>02a42007</th>\n",
       "      <th>0330ab6a</th>\n",
       "      <th>0413e89d</th>\n",
       "      <th>04df9b66</th>\n",
       "      <th>05ad839b</th>\n",
       "      <th>...</th>\n",
       "      <th>world_3_Assessment_count</th>\n",
       "      <th>world_3_Clip_count</th>\n",
       "      <th>world_3_Game_count</th>\n",
       "      <th>world_3_accumulated_correct</th>\n",
       "      <th>world_3_accumulated_incorrect</th>\n",
       "      <th>world_3_accumulated_interim_accuracy</th>\n",
       "      <th>world_3_accuracy1</th>\n",
       "      <th>world_3_action_count</th>\n",
       "      <th>world_3_mean_game_accuracy</th>\n",
       "      <th>world_3_total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>410</td>\n",
       "      <td>0.301587</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>906</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>993</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>1428</td>\n",
       "      <td>0.470238</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.198675</td>\n",
       "      <td>1854</td>\n",
       "      <td>0.512401</td>\n",
       "      <td>1854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 618 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  0086365d  00c73085  01ca3a3c  022b4259  02a42007  0330ab6a  0413e89d  \\\n",
       "0  0         0         0         0         0        23         0         0   \n",
       "1  0         0         0         0        15        23         0         0   \n",
       "2  1         0         0         0        15        23         0         0   \n",
       "3  2         0         0         0        15        70         0        14   \n",
       "4  2         0         0         0        24        70         0        14   \n",
       "\n",
       "   04df9b66  05ad839b  ...  world_3_Assessment_count  world_3_Clip_count  \\\n",
       "0         0         0  ...                         0                   7   \n",
       "1         0         0  ...                         1                  10   \n",
       "2         0         0  ...                         2                  10   \n",
       "3         0         0  ...                         4                  16   \n",
       "4         0         0  ...                         5                  20   \n",
       "\n",
       "   world_3_Game_count  world_3_accumulated_correct  \\\n",
       "0                   3                            0   \n",
       "1                   5                            1   \n",
       "2                   5                            1   \n",
       "3                   6                            2   \n",
       "4                   8                            3   \n",
       "\n",
       "   world_3_accumulated_incorrect  world_3_accumulated_interim_accuracy  \\\n",
       "0                              0                                   NaN   \n",
       "1                              0                              0.833333   \n",
       "2                             11                              0.484848   \n",
       "3                             11                              0.656566   \n",
       "4                             12                              0.659091   \n",
       "\n",
       "   world_3_accuracy1  world_3_action_count  world_3_mean_game_accuracy  \\\n",
       "0           0.000000                   410                    0.301587   \n",
       "1           0.909091                   906                    0.364286   \n",
       "2           0.082645                   993                    0.364286   \n",
       "3           0.152672                  1428                    0.470238   \n",
       "4           0.198675                  1854                    0.512401   \n",
       "\n",
       "   world_3_total_time  \n",
       "0                 410  \n",
       "1                 906  \n",
       "2                 993  \n",
       "3                1428  \n",
       "4                1854  \n",
       "\n",
       "[5 rows x 618 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление откровенно \"мусорных\" признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 features removed!\n",
      "['01ca3a3c', '0ce40006', '119b5b02', '13f56524', '16667cc5', '1b54d27f', '26a5a3dd', '29a42aea', '2ec694de', '30df3273', '3a4be871', '5f5b2617', '6077cc36', '611485c5', '6f445b57', '7fd1ac25', '9554a50b', 'ab4ec3a4', 'bfc77bd6', 'dcb1663e', 'e4d32835', 'ecc6157f', 'world_2_Activity_count', 'world_2_Assessment_count', 'world_2_Game_count', 'world_2_accumulated_correct', 'world_2_accumulated_incorrect', 'world_2_accumulated_interim_accuracy', 'world_2_accuracy1', 'world_2_mean_game_accuracy']\n"
     ]
    }
   ],
   "source": [
    "#убираем все признаки, которые имеют постоянные значения более чем для 99% наблюдений\n",
    "del_cols = []\n",
    "for col in train_wide.columns.values:\n",
    "    if len(train_wide[col].value_counts())>0:\n",
    "        counts = train_wide[col].value_counts().iloc[0]\n",
    "        if (counts / train_wide.shape[0]) >= 0.99:\n",
    "            del_cols.append(col)\n",
    "    else:\n",
    "        del_cols.append(col)\n",
    "print(str(len(del_cols)) + \" features removed!\")\n",
    "train_wide.drop(del_cols, inplace = True, axis = \"columns\")\n",
    "print(del_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15341bf18b94d30a46e9ba8180ec8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=588.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1340b8d7': ['4220'],\n",
       " '155f62a4': ['5b49460a'],\n",
       " '1996c610': ['4031'],\n",
       " '1c178d24': ['250513af'],\n",
       " '1cf54632': ['99abe2bb', '99ea62f3'],\n",
       " '2040': ['dcaede90'],\n",
       " '2050': ['37c53127'],\n",
       " '222660ff': ['38074c54'],\n",
       " '27253bdc': ['Clip'],\n",
       " '3afde5dd': ['b012cd7f'],\n",
       " '3bb91dda': ['c54cf6c5'],\n",
       " '3bfd1a65': ['db02c830'],\n",
       " '3dfd4aa4': ['83c6c409'],\n",
       " '4050': ['a1192f43'],\n",
       " '4230': ['4235', '85de926c', 'ad148f58'],\n",
       " '45d01abe': ['7525289a'],\n",
       " '48349b14': ['cc5087a3'],\n",
       " '4b5efe37': ['b7dc8128'],\n",
       " '5000': ['a6d66e51'],\n",
       " '5010': ['71e712d8'],\n",
       " '51311d7a': ['c6971acf'],\n",
       " '58a0de5c': ['9b4001e4'],\n",
       " '65a38bf7': ['7ad3efc6'],\n",
       " '736f9581': ['9b23e8ee'],\n",
       " '89aace00': ['e5734469'],\n",
       " '90d848e0': ['world_1_Assessment_count'],\n",
       " 'Welcome to Lost Lagoon!': ['world_2_Clip_count',\n",
       "  'world_2_action_count',\n",
       "  'world_2_total_time'],\n",
       " 'b120f2ac': ['c277e121'],\n",
       " 'b2e5b0f1': ['b74258a0', 'ecaab346'],\n",
       " 'world_0_action_count': ['world_0_total_time'],\n",
       " 'world_1_action_count': ['world_1_total_time'],\n",
       " 'world_3_action_count': ['world_3_total_time']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#определяем и убираем все признаки, которые имеют одинаковые значения более чем для 99% наблюдений\n",
    "same_features = {}\n",
    "counter = 0 \n",
    "for i_col in tqdm(train_wide.columns.values, total = len(train_wide.columns.values)):\n",
    "    for j_col in train_wide.columns.values:\n",
    "        if i_col == j_col:\n",
    "            continue\n",
    "        if i_col in same_features:\n",
    "            if j_col in same_features[i_col]:\n",
    "                continue\n",
    "        if j_col in same_features:\n",
    "            if i_col in same_features[j_col]:\n",
    "                continue\n",
    "        same = False\n",
    "        for col in same_features:\n",
    "            if i_col in same_features[col] and j_col in same_features[col]:\n",
    "                same = True\n",
    "        if same:\n",
    "            continue\n",
    "        same_amount = np.sum((train_wide[i_col] == train_wide[j_col]).astype(int)) / train_wide.shape[0]\n",
    "        if same_amount >= 0.99:\n",
    "            if not i_col in same_features:\n",
    "                same_features[i_col] = []\n",
    "            same_features[i_col].append(j_col)\n",
    "same_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4220',\n",
       " '5b49460a',\n",
       " '4031',\n",
       " '250513af',\n",
       " '99abe2bb',\n",
       " '99ea62f3',\n",
       " 'dcaede90',\n",
       " '37c53127',\n",
       " '38074c54',\n",
       " 'Clip',\n",
       " 'b012cd7f',\n",
       " 'c54cf6c5',\n",
       " 'db02c830',\n",
       " '83c6c409',\n",
       " 'a1192f43',\n",
       " '4235',\n",
       " '85de926c',\n",
       " 'ad148f58',\n",
       " '7525289a',\n",
       " 'cc5087a3',\n",
       " 'b7dc8128',\n",
       " 'a6d66e51',\n",
       " '71e712d8',\n",
       " 'c6971acf',\n",
       " '9b4001e4',\n",
       " '7ad3efc6',\n",
       " '9b23e8ee',\n",
       " 'e5734469',\n",
       " 'world_1_Assessment_count',\n",
       " 'world_2_Clip_count',\n",
       " 'world_2_action_count',\n",
       " 'world_2_total_time',\n",
       " 'c277e121',\n",
       " 'b74258a0',\n",
       " 'ecaab346',\n",
       " 'world_0_total_time',\n",
       " 'world_1_total_time',\n",
       " 'world_3_total_time']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop2 = []\n",
    "for value in same_features.values():\n",
    "    drop2+=value\n",
    "drop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wide.drop(drop2, inplace = True, axis = \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление избыточных признаков (спорно)\n",
    "\n",
    "Оставил ради простоты эксперимента в будущем. Вообще, был свидетелем небольшого улучшения CV при уменьшении LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17687, 550)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалим признаки с высокой корреляцией\n",
    "# TODO: при попарном удалении признаков хорошо бы удалять наименее \"осмысленные\"... \n",
    "#  по принципу формирования, например...\n",
    "\n",
    "if DROP_HIGHLY_CORRELATED:\n",
    "    \n",
    "    drop_features = set()\n",
    "    for i, candidate_feature in enumerate(train_wide.columns):\n",
    "        # print('Considering', candidate_feature)\n",
    "        for existing_feature in train_wide.columns[:i]:\n",
    "            if existing_feature not in drop_features:\n",
    "                try:\n",
    "                    cr = train_wide[candidate_feature].corr(train_wide[existing_feature])\n",
    "                    if cr > 0.99:\n",
    "                        drop_features.add(candidate_feature)\n",
    "                        print('Dropping feature', candidate_feature, existing_feature, train_wide[candidate_feature].corr(train_wide[existing_feature]))\n",
    "                        break\n",
    "                except:\n",
    "                    print('Error', existing_feature, candidate_feature)    \n",
    "\n",
    "    print('Dropping due to high correlation:', drop_features)\n",
    "                    \n",
    "    train_wide.drop(columns=list(drop_features), inplace=True)\n",
    "    \n",
    "train_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17687, 550)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удаление признаков, \"рекомендованных\" процедурой рекурсивной чистки\n",
    "# по важности\n",
    "if DROP_UNIMPORTANT:\n",
    "    unimportant_features = [\n",
    "         '67aa2ada', '6f8106d9', '15ba1109', 'd06f75b5', '0d18d96c', \n",
    "         '731c0cbe', '3b2048ee', 'Treasure Map', 'd2659ab4', '8ac7cce4',\n",
    "         '77c76bc5', '48349b14', '1cf54632', '4e5fc6f5', 'd9c005dd',\n",
    "         '6088b756', '29f54413', '29bdd9ba', 'ecc36b7f', '3bb91dda',\n",
    "         '8f094001', '7423acbc', 'e64e2cfd', '895865f3', '9b01374f',\n",
    "         'e720d930', '3ddc79c3', '53c6e11a', 'eb2c19cd', '06372577',\n",
    "         '756e5507', '46b50ba8', '0413e89d', '47f43a44', '5be391b5',\n",
    "         '51311d7a', '1beb320a', 'cb1178ad', '3bb91ced', 'e04fb33d',\n",
    "         '65abac75', '92687c59', 'd88ca108', '6aeafed4', '8d748b58',\n",
    "         '87d743c1', '7cf1bc53', '9e6b7fb5', 'world_1_accumulated_correct', '16dffff1',\n",
    "         # 50\n",
    "         'fd20ea40', '28ed704e', '2',  'c189aaf2', '3323d7e9',\n",
    "         '4b5efe37', 'd38c2fd7', '90ea0bac',  'abc5811c', '28520915',\n",
    "         'daac11b0', 'c7f7f0e1', '6f4adc4b',  '7d5c30a2', 'f56e0afc',\n",
    "         '08ff79ad', '9c5ef70c', '2070',  'world_3_accumulated_correct', '88d4a5be',\n",
    "         '736f9581', 'b5053438', 'a5be6304', '3bfd1a65', '0086365d',\n",
    "         '2a512369', 'world_0_Assessment_count', '1', '2035', '3dfd4aa4',\n",
    "         '2050', 'ea321fb1', 'cf7638f3', '63f13dd7', '1f19558b',\n",
    "         '6c930e6e', 'e37a2b78', 'a29c5338', '55115cbd', '5000',\n",
    "         '155f62a4', '5859dfb6', 'world_0_Activity_count', '4095', '14de4c5d',\n",
    "         '2b9272f4', 'a592d54e', '58a0de5c', '9ce586dd', '7f0836bf',\n",
    "        \n",
    "         #############################   100    ########################\n",
    "        \n",
    "         'd51b1749', 'c2baf0bd', '9d29771f', '1575e76c', 'b1d5101d',\n",
    "         '7d093bf9', '85d1b0de', '532a2afb', '8d84fa81', '33505eae',\n",
    "         'Heavy, Heavier, Heaviest', '5a848010', '4230', 'ec138c1c', 'f32856e4',\n",
    "         '84b0e0c8', '86c924c4', '4a09ace1', 'e7e44842', '3d0b9317',\n",
    "         'd3268efa', '7ab78247', '5010', '0d1da71f', 'Balancing Act',\n",
    "         'world_3_Assessment_count', '6cf7d25c', '857f21c0', '160654fd', 'e3ff61fb',\n",
    "        \n",
    "         # ver 7\n",
    "         'world_0_Game_count', 'ad2fc29c', '08fd73f3', 'e080a381', 'dcb55a27',\n",
    "         'e7561dd2', '2040', '4110', 'ea296733', 'b80e5e84',\n",
    "         '2060', '795e4a37', '3days_correct_assessment_attempts', '828e68f9', 'ca11f653',\n",
    "         '7961e599', 'bdf49a58', 'd3640339', '86ba578b', '19967db1',\n",
    "         # 150\n",
    "         'ac92046e', '14days_correct_assessment_attempts', 'world_3_Game_count', '77261ab5', 'a0faea5d',\n",
    "         'bc8f2793', '4d6737eb', 'e57dd7af', '6043a2b4', '71fe8f75',\n",
    "         # ver 8\n",
    "         '12 Monkeys', 'Costume Box', 'a1e4395d', '93edfe2e', 'f806dc10',\n",
    "         '5c3d2b2f', '832735e1', '90d848e0', 'world_0_accumulated_incorrect', '4080',\n",
    "         '022b4259', '25fa8af4', '1375ccb7', '17113b36', '2b058fe3',\n",
    "         '15eb4a7d', 'f71c4741', '5d042115', '4050', '804ee27f',\n",
    "         'c1cac9a2', '3dcdda7f', '2c4e6db0', '28f975ea', '7ec0c298',\n",
    "         '49ed92e9', '7days_assessment_count', '73757a5e', '65a38bf7', '2075',\n",
    "         # ver 9\n",
    "         'world_1_Activity_count', 'd122731b', 'd2e9262e', '3ccd3f02', '05ad839b',\n",
    "         '28a4eb9a', '69fdac0a', '6d90d394', '2dcad279', 'df4fe8b6',\n",
    "        \n",
    "         #############################   200  #################################\n",
    "        \n",
    "         '4c2ec19f', '2a444e03', '4901243f', '46cd75b4', '2081',\n",
    "         '3', 'c74f40cd', 'b7530680', '598f4598', 'fbaf3456',  \n",
    "         '4045', 'a1bbe385',  '67439901',  'c0415e5c',  'e79f3763',\n",
    "         'Honey Cake', '709b1251', '763fc34e', '37937459',  '6f4bd64e',\n",
    "         # ver 10\n",
    "         '3d8c61b0', '44cb4907', 'late', 'world_3_accumulated_incorrect', 'world_0_accumulated_correct',\n",
    "         'Slop Problem', '5e109ec3', '923afab1', '2230fab4', 'Rulers',\n",
    "         'a5e9da97', '77ead60d', 'a8a78786', 'f5b8c21a', '4ef8cdd3',\n",
    "         'fcfdffb6', 'Bubble Bath', 'bcceccc6', '7days_correct_assessment_attempts', 'a52b92d5',\n",
    "         'd88e8f25', 'e9c52111', 'accumulated_correct_attempts', '0a08139c', 'Egg Dropper (Activity)',\n",
    "         '4bb2f698', 'b2e5b0f1', '31973d56', '9e4c8c7b', 'a8876db3',\n",
    "        \n",
    "         # 250 \n",
    "        \n",
    "         # ver 11, 12 - пропущены из-за ошибок\n",
    "         # ver 13\n",
    "         '792530f8', 'Air Show', 'Leaf Leader', 'cdd22e43', '3babcb9b',\n",
    "         'f54238ee', '3days_assessment_count', '26fd2d99', '5f0eb72c', 'cfbd47c8',\n",
    "         '30614231', 'f93fc684', '8d7e386c', '47efca07', '14days_assessment_count',\n",
    "         '15f99afc', '2dc29e21', '3d63345e', 'beb0a7b9', 'Bug Measurer (Activity)',\n",
    "         'game_Bubble Bath_accuracy_last', '2080', '90efca10', '070a5291', 'Bottle Filler (Activity)',\n",
    "         '8af75982', 'Bird Measurer (Assessment)', 'bd612267', 'ab3136ba', 'Crystals Rule',        \n",
    "\n",
    "         # ver 14\n",
    "         'f7e47413',  'Dino Dive',  'world_3_Activity_count',  '262136f4',  '6c517a88',\n",
    "         'Flower Waterer (Activity)',  '5c2f29ca',  'Game',   'bd701df8',   '1c178d24',\n",
    "         'Magma Peak - Level 1',   '7days_incorrect_assessment_attempts',   'c7fe2a55',   'df4940d3',    '3edf6747',\n",
    "         '0330ab6a',   '93b353f2',   '4d911100',   '1cc7cfca',   'f3cd5473',\n",
    "\n",
    "         #############################   300   ###############################\n",
    "        \n",
    "         '56cd3b43',   'Magma Peak - Level 2',   'e5c9df6f',   'Assessment',   '8fee50e2',\n",
    "         'a16a373e',   '4021',   'world_1_Game_count',   'Welcome to Lost Lagoon!',   'Ordering Spheres',\n",
    "\n",
    "         # ver 15\n",
    "         '9d4e7b25',  'Tree Top City - Level 3',   '3days_incorrect_assessment_attempts',   'Lifting Heavy Things',    '1996c610',\n",
    "         'game_Leaf Leader_accuracy_last',   'Happy Camel',   '1bb5fbdb',   '7dfe6d8a',   '00c73085',\n",
    "         'Chicken Balancer (Activity)',   'd02b7a8e',   'a76029ee',   '4a4c3d21',   '9ed8f6da',\n",
    "         '3393b68b',   '2083',   '8b757ab8',   'game_Crystals Rule_accuracy_mean',   '56817e2b',\n",
    "         '91561152',   'f28c589a',   '884228c8',   '9e34ea74',   'd2278a3b',\n",
    "         '36fa3ebe',   '45d01abe',   'Chow Time',   '9ee1c98c',   'd3f1e122',\n",
    "\n",
    "         # ver16\n",
    "         'c7128948', '3days_accumulated_assessment_accuracy',  '5de79a6a',  'Pan Balance',  'Tree Top City - Level 2',\n",
    "         '89aace00', '222660ff',  'Tree Top City - Level 1',  'a7640a16',  'world_1_accuracy1',\n",
    "        \n",
    "         # 350\n",
    "        \n",
    "         'game_Scrub-A-Dub_accuracy_last',  'de26c3a6',  'b88f38da',  '2025',  '9de5e594',\n",
    "         \"Pirate's Tale\", '4010', '02a42007',  'Activity',  '5e3ea25a',\n",
    "         'a2df0760', 'c952eb01', 'acc_Mushroom Sorter (Assessment)',  '7days_accumulated_assessment_accuracy', '392e14df',\n",
    "         'Sandcastle Builder (Activity)', 'Scrub-A-Dub',  '2fb91ec1',  'game_All Star Sorting_accuracy_last', '37db1c2f',\n",
    "\n",
    "         # Результат работы RFECV, \"натравленного\" на версию 547, показал, что наибольшее значение CV \n",
    "         # (PARANOIDAL) достигается где-то при 100-150 признаках. То есть, из 532 признаков той модели\n",
    "         # удалить нужно от 380 до 430. Берем по нижней границе.\n",
    "        \n",
    "         # ver17\n",
    "         #'Dino Drink', 'current_world_Game_count', 'acc_Cauldron Filler (Assessment)', 'Crystal Caves - Level 1',  'game_Happy Camel_accuracy_last',\n",
    "         #'accumulated_uncorrect_attempts', '565a3990',  'game_Dino Drink_accuracy_last', '0db6d71d', '363d3849',\n",
    "         #'76babcde', 'b120f2ac', 'Mushroom Sorter (Assessment)', 'game_Chow Time_accuracy_mean', '2010',\n",
    "         #'bb3e370b', 'acf5c23f', '14days_actions', '5154fc30', '14days_accumulated_assessment_accuracy',\n",
    "         #'a44b10dc', '56bcd38d', 'cb6010f8', 'e4f1efe6', 'game_Air Show_accuracy_last',\n",
    "         #'461eace6', 'b2dba42b', 'game_Bubble Bath_accuracy_mean', 'd185d3ea', '1af8be29',\n",
    "\n",
    "    ]\n",
    "    print('Dropping unimportant features:', unimportant_features)\n",
    "    train_wide.drop(columns=unimportant_features, inplace=True)\n",
    "\n",
    "train_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо, сохраним предобработанные данные для последующего использования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PROCESSED_DATASETS:\n",
    "    with open('train_processed.pkl', 'wb') as fo:\n",
    "        pickle.dump(train, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['session_title', 'world']\n",
    "# Признаки, которые не нужно использовать при обучении\n",
    "exclude_features = ['installation_id', 'timestamp',\n",
    "                    'accuracy_group',  'num_correct', 'num_incorrect', 'accuracy']\n",
    "# Целевой признак\n",
    "target_feature = 'accuracy_group'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Определение требуемой метрики 'quadratic weighted kappa'\n",
    "scorer = lambda estimator, x, y : cohen_kappa_score(estimator.predict(x), y, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def qwk(a1, a2):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n",
    "\n",
    "    :param a1:\n",
    "    :param a2:\n",
    "    :param max_rat:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']\n",
    "    \n",
    "    \n",
    "class MultistartOptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coefs = [[0.5, 1.5, 2.5],\n",
    "                         [1.1, 1.6, 2.2],\n",
    "                         [1.2, 1.7, 2.1]]\n",
    "        best_val = 10.0                    # значение берется с обратным знаком\n",
    "        best_coef = None\n",
    "        for coef in initial_coefs:\n",
    "            cur_coef = sp.optimize.minimize(loss_partial, coef, method='nelder-mead')\n",
    "            cur_val = self._kappa_loss(cur_coef['x'], X, y)\n",
    "            print('Val:', cur_val)\n",
    "            if cur_val < best_val:         # значение берется с обратным знаком\n",
    "                print('New best!')\n",
    "                best_val = cur_val\n",
    "                best_coef = cur_coef\n",
    "        self.coef_ = best_coef\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Строим Нейронную сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## это нужно для того, чтобы эмбеддинге pytorch правильно построились\n",
    "train['session_title'] = train['session_title'].map({4: 0, 8: 1, 9: 2, 10: 3, 30: 4})\n",
    "train['world'] = train['world'].map({0: 0, 1: 1, 3: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## пока что заполняем  nans - '-1'. Потом, возможно, можно это сделать более умным способом.\n",
    "train.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## шкалируем данные\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "std_scal = StandardScaler()\n",
    "numerical_columns = list(set(train.columns) \\\n",
    "                         - set(categorical_features) - set(exclude_features)) \n",
    "train[numerical_columns] = std_scal.fit_transform(train[numerical_columns].values.astype(np.float64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#эмбеддинги для категориальных признаков, которые имеют более 2 уникальных значений\n",
    "embedded_cols =  {col_name: len(train[col_name].unique()) for n, col_name in enumerate(categorical_features) if train[categorical_features[n]].nunique() > 2}\n",
    "embedded_col_names = embedded_cols.keys() # название фичей, для которых будут созданы эмбеддинги\n",
    "embedding_sizes = [(c, min(50, (c+1)//2)) for _,c in embedded_cols.items()] #размеры эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание датасета:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSBDataset(Dataset):\n",
    "    def __init__(self, X, Y=None, embedded_col_names=None):\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        if Y is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = Y.values.astype(float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X1[idx], self.X2[idx]\n",
    "        else:\n",
    "            return self.X1[idx], self.X2[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Решил пока оставить только CPU, так как в кернеле все равно нельзя использовать GPU\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available() & USE_GPU:\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## переносим данные на девайс\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Количество батчей\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура нейронной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DSBModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont,\n",
    "                 input_size_list = [350, 250, 200, 100, 1],\n",
    "                 dropout_list = [0.3, 0.5, 0.5, 0.5, 0.5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) # длина всех эмбеддингов\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.layer_list = [nn.Linear(self.n_emb + self.n_cont,\n",
    "                                     input_size_list[0])]\n",
    "        self.batchnorm_list = [nn.BatchNorm1d(self.n_cont)]\n",
    "        self.dropout_list = [nn.Dropout(dropout_list[0])]\n",
    "        for i in range(0, len(input_size_list[:-1])):\n",
    "            self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
    "            self.batchnorm_list.append(nn.BatchNorm1d(input_size_list[i]))\n",
    "            self.layer_list.append(nn.Linear(input_size_list[i],\n",
    "                                             input_size_list[i+1]))\n",
    "        self.layer_list = nn.ModuleList(self.layer_list)\n",
    "        self.batchnorm_list = nn.ModuleList(self.batchnorm_list)\n",
    "        self.dropout_list = nn.ModuleList(self.dropout_list)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x2 = self.batchnorm_list[0](x_cont)\n",
    "        x = self.dropout_list[0](x)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.leaky_relu(self.layer_list[0](x), negative_slope=0.002)\n",
    "        for i in range(1, len(self.layer_list)):\n",
    "            x = self.batchnorm_list[i](x)\n",
    "            x = self.dropout_list[i](x)\n",
    "            x = F.leaky_relu(self.layer_list[i](x), negative_slope=0.002)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## \"Early stopping\" - завершение обучения модели при неуменьшении лосса на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model_params_list, experiment_name, epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model_params_list, experiment_name, epoch)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model_params_list, experiment_name, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, experiment_name, epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "\n",
    "\n",
    "#             model_full_state = {'model': model_param_list[0].state_dict()}\n",
    "                                #'optim': model_param_list[1].state_dict()}\n",
    "\n",
    "            # if model_param_list[2] is not None:\n",
    "            #     model_full_state.update({'Scheduler' : model_param_list[2].state_dict()})\n",
    "            # else:\n",
    "            #     model_full_state.update({'Scheduler': None})\n",
    "\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перепишем ClassifierWrapper:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "class ClassifierWrapperNN():\n",
    "    def __init__(self, model, optimizer,\n",
    "                 scheduler=None,\n",
    "                 batch_size=750, valid_size=0.2,\n",
    "                 embedded_col_names=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.model = model.cuda()\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.optimized_rounder = MultistartOptimizedRounder()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        ## размер валидационной выборки при обучении\n",
    "        self.valid_size = valid_size\n",
    "        ## признаки, для которых нужны эмбеддинги\n",
    "        self.embedded_col_names = embedded_col_names\n",
    "        \n",
    "        self.early_stopping = EarlyStopping(10)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=15):\n",
    "        ## нужна еще одна валидационная выборка, иначе оценка модели не будет честной\n",
    "        without_valid_set = None\n",
    "        if X_val is None:\n",
    "            valid_ids = (np.random.choice(X.installation_id.unique(),\n",
    "                                  int(len(X)*self.valid_size)))\n",
    "            valid_idx = (X[X.installation_id.isin(valid_ids)].index)\n",
    "            train_idx = np.delete(X.index, valid_idx)\n",
    "            X_train = X.iloc[train_idx, :].drop(columns='installation_id')\n",
    "            y_train = y.iloc[train_idx]\n",
    "            X_val = X.iloc[valid_idx, :].drop(columns='installation_id')\n",
    "            y_val = y.iloc[valid_idx]\n",
    "            without_valid_set = True\n",
    "        else:\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "        ## устройство, на котором будет производиться обучение:\n",
    "        self.device = get_default_device()\n",
    "        ## инициализируем датасет и загрузчик данных\n",
    "        train_ds = DSBDataset(X_train, y_train, self.embedded_col_names)\n",
    "        valid_ds = DSBDataset(X_val, y_val, self.embedded_col_names)\n",
    "        train_dl = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "        valid_dl = DataLoader(valid_ds, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "        ## перенос данных на нужное устройство:\n",
    "        self.train_dl = DeviceDataLoader(train_dl, device)\n",
    "        self.valid_dl = DeviceDataLoader(valid_dl, device)\n",
    "        \n",
    "        # обучение\n",
    "        self.train_loop(epochs=epochs)\n",
    "        \n",
    "        ## Чтобы подобрать пороги предскажем ответы для тренировочных данных\n",
    "        if without_valid_set:\n",
    "            y_pred = self.predict_regressor(X.drop(columns='installation_id'))\n",
    "        else:\n",
    "            y_pred = self.predict_regressor(X)\n",
    "            \n",
    "        y_true = y.values\n",
    "        \n",
    "        self.optimized_rounder.fit(y_pred.reshape(-1), y_true.reshape(-1))\n",
    "        print('Found bounds:', self.optimized_rounder.coefficients())\n",
    "    \n",
    "    def predict_regressor(self, X):\n",
    "        ## выдает вещественные ответы для X\n",
    "        \n",
    "        valid_ds = DSBDataset(X, Y=None, embedded_col_names=self.embedded_col_names)\n",
    "        valid_dl = DataLoader(valid_ds, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "        valid_dl = DeviceDataLoader(valid_dl, device)\n",
    "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "        self.model.eval()\n",
    "        preds = None\n",
    "        with torch.no_grad():\n",
    "            for x1, x2 in valid_dl:\n",
    "                current_batch_size = x2.shape[0]\n",
    "                out = self.model(x1, x2)\n",
    "                if preds is None:\n",
    "                    preds = out.cpu().numpy()\n",
    "                else:\n",
    "                    preds = np.hstack([preds.reshape(-1), out.cpu().numpy().reshape(-1)])\n",
    "        return preds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_regressor(X)\n",
    "        return self.optimized_rounder.predict(y_pred.reshape(-1),\n",
    "                                              self.optimized_rounder.coefficients())\n",
    "    \n",
    "    def train_loop(self, epochs):\n",
    "        for i in range(epochs): \n",
    "            loss = self.train_model()\n",
    "            print(f\"training loss: {loss:.3f}\")\n",
    "            val_loss = self.val_loss()\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(val_loss)\n",
    "            self.early_stopping(val_loss, self.model, None, None)\n",
    "            if self.early_stopping.early_stop:\n",
    "                break\n",
    "        del self.train_dl, self.valid_dl\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.model.train()\n",
    "        loss_list = []\n",
    "        for x1, x2, y in self.train_dl:\n",
    "            batch = y.shape[0]\n",
    "            out = self.model(x1, x2)\n",
    "            loss = torch.sqrt(F.mse_loss(out.reshape(-1), y.reshape(-1).float()))   \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_list.append((loss.item()))\n",
    "        return np.mean(loss_list)\n",
    "    \n",
    "    def val_loss(self):\n",
    "        self.model.eval()\n",
    "        loss_list = []\n",
    "        correct = 0\n",
    "        preds = None\n",
    "        y_true = None\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y in self.valid_dl:\n",
    "                current_batch_size = y.shape[0]\n",
    "                out = self.model(x1, x2)\n",
    "                loss = torch.sqrt(F.mse_loss(out.reshape(-1), y.reshape(-1).float()))\n",
    "                loss_list.append((loss.item()))\n",
    "                if y_true is None:\n",
    "                    y_true = y.cpu()\n",
    "                else:\n",
    "                    y_true = np.hstack([y_true.reshape(-1), y.cpu().reshape(-1).float()])\n",
    "                if preds is None:\n",
    "                    preds = out.cpu().numpy()\n",
    "                else:\n",
    "                    preds = np.hstack([preds.reshape(-1), out.cpu().numpy().reshape(-1)])\n",
    "        #self.optimized_rounder.fit(preds.reshape(-1,), y_true)\n",
    "        #kappa_score = qwk(self.optimized_rounder.predict(preds.reshape(-1), \n",
    "        #                                                 self.optimized_rounder.coefficients()).astype(int),\n",
    "        #                  y_true)\n",
    "        print(f\"valid loss {np.mean(loss_list):.3f}\")# and kappa {kappa_score:.3f}\")\n",
    "        return np.mean(loss_list)#, kappa_score\n",
    "\n",
    "\n",
    "# class ClassifierWrapper1(BaseEstimator, MetaEstimatorMixin, ClassifierMixin):\n",
    "    \n",
    "#     def __init__(self, base_estimator):\n",
    "#         super().__init__()\n",
    "#         self.base_estimator = base_estimator\n",
    "#         self.optimized_rounder = MultistartOptimizedRounder()\n",
    "\n",
    "#     def make_estimator(self):\n",
    "#         estimator = clone(self.base_estimator)\n",
    "#         return estimator        \n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "#         self.model = self.make_estimator().fit(X, y)\n",
    "#         y_pred = self.model.predict(X)\n",
    "#         self.optimized_rounder.fit(y_pred.reshape(-1,), y)\n",
    "#         print('Found bounds:', self.optimized_rounder.coefficients())\n",
    "        \n",
    "#     def predict(self, X):\n",
    "#         y_pred = self.model.predict(X)\n",
    "#         return self.optimized_rounder.predict(y_pred.reshape(-1, ), self.optimized_rounder.coefficients())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DSBModel(embedding_sizes=embedding_sizes, \n",
    "                 n_cont=len(train.drop(columns=exclude_features).columns) - len(categorical_features))\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "lr = 0.03\n",
    "wd = 1e-6\n",
    "optimizer = optim.Adam(parameters, lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_model = ClassifierWrapperNN(model=model, optimizer=optimizer,\n",
    "                                      embedded_col_names=embedded_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## для воспроизводимости результатов\n",
    "import random\n",
    "\n",
    "def seed_torch(seed=13):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # если используется несколько GPU\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "\n",
    "seed_torch(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.904\n",
      "valid loss 2.190\n",
      "Validation loss decreased (inf --> 2.189767).  Saving model ...\n",
      "training loss: 1.545\n",
      "valid loss 1.434\n",
      "Validation loss decreased (2.189767 --> 1.433925).  Saving model ...\n",
      "training loss: 1.505\n",
      "valid loss 1.295\n",
      "Validation loss decreased (1.433925 --> 1.295136).  Saving model ...\n",
      "training loss: 1.326\n",
      "valid loss 1.223\n",
      "Validation loss decreased (1.295136 --> 1.223295).  Saving model ...\n",
      "training loss: 1.330\n",
      "valid loss 1.167\n",
      "Validation loss decreased (1.223295 --> 1.166604).  Saving model ...\n",
      "training loss: 1.211\n",
      "valid loss 1.166\n",
      "Validation loss decreased (1.166604 --> 1.166253).  Saving model ...\n",
      "training loss: 1.218\n",
      "valid loss 1.232\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.221\n",
      "valid loss 1.179\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.216\n",
      "valid loss 1.171\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.312\n",
      "valid loss 1.148\n",
      "Validation loss decreased (1.166253 --> 1.148442).  Saving model ...\n",
      "training loss: 1.173\n",
      "valid loss 1.164\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.456\n",
      "valid loss 1.176\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.223\n",
      "valid loss 1.169\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.216\n",
      "valid loss 1.181\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 1.186\n",
      "valid loss 1.152\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-7e6567382b89>:3: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"qwk\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-33-7e6567382b89> (13)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 13:\u001b[0m\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "\u001b[1m    max_rat = 3\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-33-7e6567382b89>:3: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"qwk\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 21:\u001b[0m\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "    <source elided>\n",
      "    o = 0\n",
      "\u001b[1m    for k in range(a1.shape[0]):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"qwk\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 4:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 4:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: -0.4700782345057597\n",
      "New best!\n",
      "Val: -0.4886435276137999\n",
      "New best!\n",
      "Val: -0.487188926924946\n",
      "Found bounds: [1.2048009  1.97340801 2.30882332]\n",
      "CPU times: user 44.9 s, sys: 353 ms, total: 45.2 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "regressor_model.fit(train.drop(columns=exclude_features[1:]), train['accuracy_group'], epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if MULTISEED_BLEND:\n",
    "#     from sklearn.ensemble import VotingRegressor\n",
    "#     template_model = CatBoostRegressor()\n",
    "#     base_models = []\n",
    "#     for i in range(MULTISEED_BLEND_MODEL_COUNT):\n",
    "#         model = clone(template_model)\n",
    "\n",
    "#         model.set_params(**{'random_state': RANDOM_SEED+i,\n",
    "#                                         'iterations': 1000,\n",
    "#                                         'verbose': 100,\n",
    "#                                         'cat_features': categorical_features})\n",
    "#         base_models.append((f'model_{i+1}', model))\n",
    "\n",
    "#     rgr_model = ClassifierWrapper1(VotingRegressor(estimators=base_models))\n",
    "\n",
    "# else:\n",
    "    \n",
    "#     rgr_model = ClassifierWrapper(CatBoostRegressor(), {'random_state': RANDOM_SEED,\n",
    "#                                                         'iterations': 1000,\n",
    "#                                                         'verbose': 100,\n",
    "#                                                         'cat_features': categorical_features})\n",
    "\n",
    "X = train.drop(columns=exclude_features)\n",
    "y = train[[target_feature]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class OneTestPerGroupKFold:\n",
    "    \"\"\"\n",
    "    Генерация фолдов для кросс-валидации, в которых в тестовой части будет\n",
    "    только по одной записи от каждого пользователя (installation_id).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        if shuffle:\n",
    "            self.splitter = KFold(n_splits, shuffle=True, random_state=random_state)\n",
    "        else:\n",
    "            self.splitter = KFold(n_splits)\n",
    "        if random_state is None:\n",
    "            random_state = 0\n",
    "        self.random = random.Random(random_state)\n",
    "        \n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, groups):\n",
    "        # Составим вспомогательный набор с отображением атрибута\n",
    "        # группировки (installation_id) на порядковый индекс \n",
    "        # соответствующей записи\n",
    "        tt = pd.DataFrame({'group': groups, 'idx': np.arange(groups.shape[0])})\n",
    "        # Выбираем по одному представителю для каждой группы\n",
    "        picks = tt.groupby(['group'], sort=False)['idx'].agg(lambda x: x.iloc[self.random.randint(0, x.shape[0]-1)])\n",
    "        # Генерируем фолды по группам (пользователям)\n",
    "        for train_idx, test_idx in self.splitter.split(picks):\n",
    "            # \"Раскроем\" индексы:\n",
    "            # Для обучающей выборки это будут все записи соответствующих групп\n",
    "            train_idx_ = tt.idx[tt.group.isin(list(picks.iloc[train_idx].index))].to_numpy(dtype='int')\n",
    "            # Для тестовой выборки это будут только выбранные записи из соответствующих групп\n",
    "            test_idx_ = picks.iloc[test_idx].to_numpy(dtype='int')\n",
    "            yield (train_idx_, test_idx_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "def evaluate_featureset(X, y, cv, groups=train.installation_id, model_count=2, OneTestPerGroup=False,\n",
    "                        nn_params=None):\n",
    "    \"\"\"\n",
    "    Функция оценки набора признаков (в первую очередь).\n",
    "    OneTestPerGroup = True, если хотим использовать OneTestPerGroupKFold.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    accumulated_importances = np.zeros(len(X.columns))\n",
    "    # Пробуем несколько случайных состояний\n",
    "    # for seed_delta in [17-RANDOM_SEED, 28-RANDOM_SEED, 45-RANDOM_SEED]:\n",
    "#     first_layer_size = np.max([nn_params['first_layer_size'],\n",
    "#                                    nn_params['last_layer_size']])\n",
    "#     last_layer_size = np.min([nn_params['first_layer_size'],\n",
    "#                                nn_params['last_layer_size']])\n",
    "    input_size_list = [350, 250, 200, 100, 1]#np.sort(np.random.randint(low=last_layer_size,\n",
    "                       #                 high=first_layer_size,\n",
    "                        #                size=int(nn_params['layers_amount'])))[::-1]\n",
    "    input_size_list[-1] = 1\n",
    "    dropout_list = [0.3, 0.5, 0.5, 0.5, 0.5]#np.random.rand(int(nn_params['layers_amount'])) * 0.5 + 0.3\n",
    "    for seed_delta in range(model_count):\n",
    "        print('Training model with seed +', seed_delta)\n",
    "        \n",
    "        ## смена сида для PARANOIDAL_CV\n",
    "        seed_torch(RANDOM_SEED + seed_delta)\n",
    "        if OneTestPerGroup:\n",
    "            splitter = cv.split(X, groups=groups)\n",
    "        else:\n",
    "            splitter = cv.split(X, y, groups=groups)\n",
    "        \n",
    "        for train_idxs, test_idxs in splitter:\n",
    "            model = DSBModel(embedding_sizes, len(X.columns) - len(categorical_features),\n",
    "                             input_size_list=input_size_list,\n",
    "                             dropout_list=dropout_list)\n",
    "            parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "            lr = 0.02\n",
    "            wd = 1e-6#nn_params['weights_decay']\n",
    "            optimizer = optim.Adam(parameters, lr=lr, weight_decay=wd, amsgrad=True)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                             factor=0.2,\n",
    "                                                             patience=3,\n",
    "                                                             verbose=True,\n",
    "                                                             eps=1e-8)\n",
    "            train_X = X.iloc[train_idxs, :]\n",
    "            train_y = y.iloc[train_idxs, :]\n",
    "            test_X = X.iloc[test_idxs, :]\n",
    "            test_y = y.iloc[test_idxs, :]\n",
    "\n",
    "            tmp_model = ClassifierWrapperNN(model=model, \n",
    "                                            optimizer=optimizer,\n",
    "                                            scheduler=scheduler,\n",
    "                                            embedded_col_names=embedded_col_names)\n",
    "            tmp_model.fit(train_X, train_y, test_X, test_y, 100)\n",
    "            ## К сожалению, feature importances не получить(\n",
    "#             accumulated_importances += tmp_model.model.feature_importances_\n",
    "            tmp_preds = tmp_model.predict(test_X)\n",
    "            tmp_score = qwk(tmp_preds, test_y.values.reshape(-1, ))\n",
    "            print('OOF score: ', tmp_score)\n",
    "\n",
    "            scores.append(tmp_score)\n",
    "    print(input_size_list,\n",
    "          dropout_list)\n",
    "    return np.array(scores), input_size_list, dropout_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подберем параметры с помощью bayes_opt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# best_score_and_params_ORDINARY_mean = {'score': 0,\n",
    "#                                         'layers_amount': 0,\n",
    "#                                         'weights_decay': 0,\n",
    "#                                         'first_layer_size': 0,\n",
    "#                                         'last_layer_size': 0,\n",
    "#                                         'input_size_list': 0,\n",
    "#                                         'dropout_list': 0}\n",
    "\n",
    "# best_score_and_params_ORDINARY_min = {'score': 0,\n",
    "#                                         'layers_amount': 0,\n",
    "#                                         'weights_decay': 0,\n",
    "#                                         'first_layer_size': 0,\n",
    "#                                         'last_layer_size': 0,\n",
    "#                                         'input_size_list': 0,\n",
    "#                                         'dropout_list': 0}\n",
    "\n",
    "# def nn_bayesian_ORDINARY(layers_amount, weights_decay, first_layer_size, \n",
    "#                 last_layer_size):#, subsample):\n",
    "    \n",
    "#     nn_params = {\n",
    "#                  'layers_amount': layers_amount,\n",
    "#                  'weights_decay': weights_decay,\n",
    "#                  'last_layer_size': last_layer_size,\n",
    "#                  'first_layer_size': first_layer_size}\n",
    "#     splitter = GroupKFold(ORDINARY_CV_FOLD_COUNT)\n",
    "#     cv_score, input_size_list, dropout_list = evaluate_featureset(X, y, splitter, model_count=1, \n",
    "#                                                 OneTestPerGroup=False, nn_params=nn_params)\n",
    "#     if cv_score.mean() > best_score_and_params_ORDINARY_mean['score']:\n",
    "#         best_score_and_params_ORDINARY_mean['score'] = cv_score.mean()\n",
    "#         best_score_and_params_ORDINARY_mean['layers_amount'] = nn_params['layers_amount']\n",
    "#         best_score_and_params_ORDINARY_mean['weights_decay'] = nn_params['weights_decay']\n",
    "#         best_score_and_params_ORDINARY_mean['last_layer_size'] = nn_params['last_layer_size']\n",
    "#         best_score_and_params_ORDINARY_mean['first_layer_size'] = nn_params['first_layer_size']\n",
    "#         best_score_and_params_ORDINARY_mean['input_size_list'] = input_size_list\n",
    "#         best_score_and_params_ORDINARY_mean['dropout_list'] = dropout_list\n",
    "#     if cv_score.min() > best_score_and_params_ORDINARY_min['score']:\n",
    "#         best_score_and_params_ORDINARY_min['score'] = cv_score.min()\n",
    "#         best_score_and_params_ORDINARY_min['layers_amount'] = nn_params['layers_amount']\n",
    "#         best_score_and_params_ORDINARY_min['weights_decay'] = nn_params['weights_decay']\n",
    "#         best_score_and_params_ORDINARY_min['last_layer_size'] = nn_params['last_layer_size']\n",
    "#         best_score_and_params_ORDINARY_min['first_layer_size'] = nn_params['first_layer_size']\n",
    "#         best_score_and_params_ORDINARY_min['input_size_list'] = input_size_list\n",
    "#         best_score_and_params_ORDINARY_min['dropout_list'] = dropout_list\n",
    "        \n",
    "#     cv_score = np.sort(cv_score)[1:-1]\n",
    "#     return cv_score.mean()\n",
    "\n",
    "\n",
    "# nn_params_bounds = {\n",
    "#     'layers_amount': (1, 10),\n",
    "#     'weights_decay': (1e-6, 5e-4),\n",
    "#     'first_layer_size': (300, 600),\n",
    "#     'last_layer_size': (10, 150)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import warnings\n",
    "# nn_BO = BayesianOptimization(nn_bayesian_ORDINARY, nn_params_bounds, random_state=RANDOM_SEED)\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings('ignore')\n",
    "#     nn_BO.maximize(init_points=1000, n_iter=1000, acq='ucb', xi=0, alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Оптимальные параметры ORDINARY:\\n{nn_BO.max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score_and_params_ORDINARY_mean, best_score_and_params_ORDINARY_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "({'score': 0.5747473730904427,\n",
    "  'layers_amount': 8.291512827267493,\n",
    "  'weights_decay': 3.6890671667118504e-06,\n",
    "  'first_layer_size': 454.59224937264935,\n",
    "  'last_layer_size': 67.4819895666651,\n",
    "  'input_size_list': array([391, 380, 345, 308, 307, 210, 178,   1]),\n",
    "  'dropout_list': array([0.35149693, 0.4097505 , 0.61487881, 0.72996204, 0.32063628,\n",
    "         0.67128311, 0.58520868, 0.3171566 ])},\n",
    " {'score': 0.5588338575413847,\n",
    "  'layers_amount': 9.778262752653031,\n",
    "  'weights_decay': 0.000178995134285722,\n",
    "  'first_layer_size': 518.8253842758594,\n",
    "  'last_layer_size': 52.96506511880656,\n",
    "  'input_size_list': array([481, 458, 442, 365, 330, 293, 292, 195,   1]),\n",
    "  'dropout_list': array([0.50511163, 0.35149693, 0.4097505 , 0.61487881, 0.72996204,\n",
    "         0.32063628, 0.67128311, 0.58520868, 0.3171566 ])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# best_score_and_params_PARANOIDAL_mean = {'score': 0,\n",
    "#                                         'layers_amount': 0,\n",
    "#                                         'weights_decay': 0,\n",
    "#                                         'first_layer_size': 0,\n",
    "#                                         'last_layer_size': 0,\n",
    "#                                         'input_size_list': 0,\n",
    "#                                         'dropout_list': 0}\n",
    "\n",
    "# best_score_and_params_PARANOIDAL_min = {'score': 0,\n",
    "#                                         'layers_amount': 0,\n",
    "#                                         'weights_decay': 0,\n",
    "#                                         'first_layer_size': 0,\n",
    "#                                         'last_layer_size': 0,\n",
    "#                                         'input_size_list': 0,\n",
    "#                                         'dropout_list': 0}\n",
    "\n",
    "# def nn_bayesian_PARANOIDAL(layers_amount, weights_decay, first_layer_size, \n",
    "#                 last_layer_size):#, subsample):\n",
    "    \n",
    "#     nn_params = {\n",
    "#                  'layers_amount': layers_amount,\n",
    "#                  'weights_decay': weights_decay,\n",
    "#                  'last_layer_size': last_layer_size,\n",
    "#                  'first_layer_size': first_layer_size}\n",
    "#     pcv_splitter = GroupKFold(PARANOIDAL_CV_FOLD_COUNT)\n",
    "#     cv_score, input_size_list, dropout_list = evaluate_featureset(X, y, pcv_splitter, model_count=PARANOIDAL_CV_SEED_COUNT, \n",
    "#                                                 OneTestPerGroup=False, nn_params=nn_params)\n",
    "#     if cv_score.mean() > best_score_and_params_PARANOIDAL_mean['score']:\n",
    "#         best_score_and_params_PARANOIDAL_mean['score'] = cv_score.mean()\n",
    "#         best_score_and_params_PARANOIDAL_mean['layers_amount'] = nn_params['layers_amount']\n",
    "#         best_score_and_params_PARANOIDAL_mean['weights_decay'] = nn_params['weights_decay']\n",
    "#         best_score_and_params_PARANOIDAL_mean['last_layer_size'] = nn_params['last_layer_size']\n",
    "#         best_score_and_params_PARANOIDAL_mean['first_layer_size'] = nn_params['first_layer_size']\n",
    "#         best_score_and_params_PARANOIDAL_mean['input_size_list'] = input_size_list\n",
    "#         best_score_and_params_PARANOIDAL_mean['dropout_list'] = dropout_list\n",
    "#     if cv_score.min() > best_score_and_params_PARANOIDAL_min['score']:\n",
    "#         best_score_and_params_PARANOIDAL_min['score'] = cv_score.min()\n",
    "#         best_score_and_params_PARANOIDAL_min['layers_amount'] = nn_params['layers_amount']\n",
    "#         best_score_and_params_PARANOIDAL_min['weights_decay'] = nn_params['weights_decay']\n",
    "#         best_score_and_params_PARANOIDAL_min['last_layer_size'] = nn_params['last_layer_size']\n",
    "#         best_score_and_params_PARANOIDAL_min['first_layer_size'] = nn_params['first_layer_size']\n",
    "#         best_score_and_params_PARANOIDAL_min['input_size_list'] = input_size_list\n",
    "#         best_score_and_params_PARANOIDAL_min['dropout_list'] = dropout_list\n",
    "        \n",
    "#     cv_score = np.sort(cv_score)[1:-1]\n",
    "#     return cv_score.mean()\n",
    "\n",
    "\n",
    "# nn_params_bounds = {\n",
    "#     'layers_amount': (1, 10),\n",
    "#     'weights_decay': (1e-6, 5e-4),\n",
    "#     'first_layer_size': (300, 600),\n",
    "#     'last_layer_size': (10, 150)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import warnings\n",
    "# nn_BO = BayesianOptimization(nn_bayesian_PARANOIDAL, nn_params_bounds, random_state=RANDOM_SEED)\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings('ignore')\n",
    "#     nn_BO.maximize(init_points=100, n_iter=100, acq='ucb', xi=0, alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Оптимальные параметры PARANOIDAL:\\n{nn_BO.max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score_and_params_PARANOIDAL_mean, best_score_and_params_PARANOIDAL_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# best_score_and_params_OneUserPerGroup_mean = {'score': 0,\n",
    "#                                         'layers_amount': 0,\n",
    "#                                         'weights_decay': 0,\n",
    "#                                         'first_layer_size': 0,\n",
    "#                                         'last_layer_size': 0,\n",
    "#                                         'input_size_list': 0,\n",
    "#                                         'dropout_list': 0}\n",
    "\n",
    "# best_score_and_params_OneUserPerGroup_min = {'score': 0,\n",
    "#                                         'layers_amount': 0,\n",
    "#                                         'weights_decay': 0,\n",
    "#                                         'first_layer_size': 0,\n",
    "#                                         'last_layer_size': 0,\n",
    "#                                         'input_size_list': 0,\n",
    "#                                         'dropout_list': 0}\n",
    "\n",
    "# def nn_bayesian_OneUserPerGroup(layers_amount, weights_decay, first_layer_size, \n",
    "#                 last_layer_size):#, subsample):\n",
    "    \n",
    "#     nn_params = {\n",
    "#                  'layers_amount': layers_amount,\n",
    "#                  'weights_decay': weights_decay,\n",
    "#                  'last_layer_size': last_layer_size,\n",
    "#                  'first_layer_size': first_layer_size}\n",
    "#     opu_splitter = OneTestPerGroupKFold(ONE_PER_USER_CV_FOLD_COUNT, shuffle=True, random_state=RANDOM_SEED)\n",
    "#     cv_score, input_size_list, dropout_list = evaluate_featureset(X, y, cv=opu_splitter,\n",
    "#                                                                   groups=train.installation_id, model_count=1,\n",
    "#                                                                   OneTestPerGroup=True, nn_params=nn_params)\n",
    "#     if cv_score.mean() > best_score_and_params_OneUserPerGroup_mean['score']:\n",
    "#         best_score_and_params_OneUserPerGroup_mean['score'] = cv_score.mean()\n",
    "#         best_score_and_params_OneUserPerGroup_mean['layers_amount'] = nn_params['layers_amount']\n",
    "#         best_score_and_params_OneUserPerGroup_mean['weights_decay'] = nn_params['weights_decay']\n",
    "#         best_score_and_params_OneUserPerGroup_mean['last_layer_size'] = nn_params['last_layer_size']\n",
    "#         best_score_and_params_OneUserPerGroup_mean['first_layer_size'] = nn_params['first_layer_size']\n",
    "#         best_score_and_params_OneUserPerGroup_mean['input_size_list'] = input_size_list\n",
    "#         best_score_and_params_OneUserPerGroup_mean['dropout_list'] = dropout_list\n",
    "#     if cv_score.min() > best_score_and_params_OneUserPerGroup_min['score']:\n",
    "#         best_score_and_params_OneUserPerGroup_min['score'] = cv_score.min()\n",
    "#         best_score_and_params_OneUserPerGroup_min['layers_amount'] = nn_params['layers_amount']\n",
    "#         best_score_and_params_OneUserPerGroup_min['weights_decay'] = nn_params['weights_decay']\n",
    "#         best_score_and_params_OneUserPerGroup_min['last_layer_size'] = nn_params['last_layer_size']\n",
    "#         best_score_and_params_OneUserPerGroup_min['first_layer_size'] = nn_params['first_layer_size']\n",
    "#         best_score_and_params_OneUserPerGroup_min['input_size_list'] = input_size_list\n",
    "#         best_score_and_params_OneUserPerGroup_min['dropout_list'] = dropout_list\n",
    "        \n",
    "#     cv_score = np.sort(cv_score)[1:-1]\n",
    "#     return cv_score.mean()\n",
    "\n",
    "\n",
    "# nn_params_bounds = {\n",
    "#     'layers_amount': (1, 10),\n",
    "#     'weights_decay': (1e-6, 5e-4),\n",
    "#     'first_layer_size': (300, 600),\n",
    "#     'last_layer_size': (10, 150)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import warnings\n",
    "# nn_BO = BayesianOptimization(nn_bayesian_OneUserPerGroup, nn_params_bounds, random_state=RANDOM_SEED)\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings('ignore')\n",
    "#     nn_BO.maximize(init_points=100, n_iter=10, acq='ucb', xi=0, alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Оптимальные параметры OneUserPerGroup:\\n{nn_BO.max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score_and_params_OneUserPerGroup_mean, best_score_and_params_OneUserPerGroup_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Чтобы ускорить процесс проверки (сабмит), кросс-валидацию при ней не проводим\n",
    "if not IS_SUBMITTING and ORDINARY_CV:\n",
    "    splitter = GroupKFold(ORDINARY_CV_FOLD_COUNT)\n",
    "    cv_score, _, _ = evaluate_featureset(X, y, splitter, model_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обычная валидация (GroupKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_SUBMITTING and ORDINARY_CV:\n",
    "    print('Ordinary CV')\n",
    "    print('=' * 20)\n",
    "    print('CV scores:', cv_score)\n",
    "    print('CV scores mean:', cv_score.mean())\n",
    "    print('CV scores std:', cv_score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary CV\n",
    "\n",
    "    CV scores: [0.56972199 0.54772569 0.58121881 0.60519197 0.56402832]\n",
    "    CV scores mean: 0.5735773557996026\n",
    "    CV scores std: 0.01914279135356409"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary CV - min (based on ordinary)\n",
    "\n",
    "    CV scores: [0.56707519 0.54249428 0.57127998 0.59669516 0.55918574]\n",
    "    CV scores mean: 0.5673460701180641\n",
    "    CV scores std: 0.0176698626840161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Референтные значения:\n",
    "\n",
    "557 (ORDINARY_CV) (https://www.kaggle.com/panikads/data-processing-consolidated-376397 версия 2):\n",
    "\n",
    "    CV scores: [0.58856511 0.58146084 0.59524134 0.62754587 0.58654045]\n",
    "    CV scores mean: 0.5958707211893111\n",
    "    CV scores std: 0.01644251752818246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Параноидальная\" валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not IS_SUBMITTING and PARANOIDAL_CV:\n",
    "    pcv_splitter = GroupKFold(PARANOIDAL_CV_FOLD_COUNT)\n",
    "    cv_score, importances, _ = evaluate_featureset(X, y, pcv_splitter, model_count=PARANOIDAL_CV_SEED_COUNT)\n",
    "    cv_score = np.sort(cv_score)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_SUBMITTING and PARANOIDAL_CV:\n",
    "    print('Paranoidal CV')\n",
    "    print('=' * 20)\n",
    "    print('CV scores:', cv_score)\n",
    "    print('CV scores mean:', cv_score.mean())\n",
    "    print('CV scores std:', cv_score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paranoidal CV\n",
    "\n",
    "    CV scores: [0.5420533  0.57608893 0.57831589 0.58084066]\n",
    "    CV scores mean: 0.5693246948078672\n",
    "    CV scores std: 0.01583463477706842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Референтные значения:\n",
    "\n",
    "557 (PARANOIDAL_CV) (https://www.kaggle.com/panikads/data-processing-consolidated-376397 версия 2):\n",
    "\n",
    "    CV scores: [0.58519314 0.58624647 0.59040344 0.60414955]\n",
    "    CV scores mean: 0.59149815295885\n",
    "    CV scores std: 0.007559587813019929    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По одному тесту на пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not IS_SUBMITTING and ONE_PER_USER_CV:\n",
    "    opu_splitter = OneTestPerGroupKFold(ONE_PER_USER_CV_FOLD_COUNT, shuffle=True, random_state=RANDOM_SEED)\n",
    "    cv_score, _, _ = evaluate_featureset(X, y, cv=opu_splitter, groups=train.installation_id, model_count=1,\n",
    "                                   OneTestPerGroup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_SUBMITTING and ONE_PER_USER_CV:\n",
    "    print('One-Test-Per-User CV')\n",
    "    print('=' * 20)\n",
    "    print('CV scores:', cv_score)\n",
    "    print('CV scores mean:', cv_score.mean())\n",
    "    print('CV scores std:', cv_score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Test-Per-User CV\n",
    "\n",
    "    CV scores: [0.56152823 0.53102847 0.58581524 0.55590726 0.58448147]\n",
    "    CV scores mean: 0.5637521344018138\n",
    "    CV scores std: 0.020267274356328507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование столбца с предсказаниями (опционально)\n",
    "\n",
    "В этом блоке формируются предсказания для набора данных (при перекрестном обучении) для анализа.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SAVE_PREDICTED_TARGETS and not IS_SUBMITTING:\n",
    "\n",
    "#     from sklearn.model_selection import cross_val_predict\n",
    "    \n",
    "#     pred_splitter = GroupKFold(5)\n",
    "#     y_pred = cross_val_predict(rgr_model, X, y, \n",
    "#                                groups=train.installation_id,\n",
    "#                                cv=pred_splitter)\n",
    "    \n",
    "#     # Сохраним предсказания для дальнейшего анализа\n",
    "#     pd.DataFrame({'pred_accuracy_group': y_pred}).to_csv('train_predictions.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if SAVE_PREDICTED_TARGETS and not IS_SUBMITTING:\n",
    "\n",
    "#     # https://www.kaggle.com/agungor2/various-confusion-matrix-plots\n",
    "#     from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#     data = confusion_matrix(y, y_pred)\n",
    "#     df_cm = pd.DataFrame(data, columns=np.unique(y), index = np.unique(y))\n",
    "#     df_cm.index.name = 'Actual'\n",
    "#     df_cm.columns.name = 'Predicted'\n",
    "#     plt.figure(figsize = (10,7))\n",
    "#     sn.set(font_scale=1.4) # for label size\n",
    "#     sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14}, fmt='d') # font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка тестового набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945035988e1a4452b02f90cc39ade261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Installation_id', max=1000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 593)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = []\n",
    "for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "    a = get_data(user_sample, test_set=True)\n",
    "    new_test.append(a)\n",
    "\n",
    "test = pd.DataFrame(new_test)\n",
    "del new_test\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = make_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 550)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop(del_cols, inplace = True, axis = \"columns\")\n",
    "test.drop(drop2, inplace = True, axis = \"columns\")\n",
    "if DROP_HIGHLY_CORRELATED:\n",
    "    test.drop(columns=list(drop_columns), inplace=True)\n",
    "if DROP_UNIMPORTANT:\n",
    "    test.drop(columns=unimportant_features, inplace=True)\n",
    "\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PROCESSED_DATASETS and not IS_SUBMITTING:\n",
    "    with open('test_processed.pkl', 'wb') as fo:\n",
    "        pickle.dump(test, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=exclude_features)\n",
    "y = train[[target_feature]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Псевдолейблинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PSEUDOLABELING:\n",
    "    \n",
    "    ## Обучим базовую модель для псевдолейблинга: \n",
    "    if USE_SOFTLABELS:\n",
    "        rgr_model_pseudo = CatBoostRegressor(random_state=RANDOM_SEED + 10,\n",
    "                                             iterations=1000,\n",
    "                                             verbose=100,\n",
    "                                             cat_features=categorical_features)\n",
    "    else:\n",
    "        rgr_model_pseudo = ClassifierWrapper(CatBoostRegressor(), {'random_state': RANDOM_SEED + 10,\n",
    "                                                                    'iterations': 1000,\n",
    "                                                                    'verbose': 100,\n",
    "                                                                    'cat_features': categorical_features})\n",
    "    X = train.drop(columns=exclude_features)\n",
    "    y = train[[target_feature]]\n",
    "    rgr_model_pseudo.fit(X, y)\n",
    "        \n",
    "    ## псевдолейблы для теста (возможен другой вариант - запоминать напрямую выход регрессионной модели\n",
    "    ## без разбиения по порогам\n",
    "    pseudolabels = rgr_model_pseudo.predict(test.drop(columns=exclude_features)).astype(float)\n",
    "    \n",
    "    ## вопрос - нужно ли перемешивать train и test в этом случае?\n",
    "    X = pd.concat([X, test.drop(columns=exclude_features)], sort=False).reset_index(drop=True)\n",
    "    y = np.hstack([y.values.reshape(-1), pseudolabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "def ensemble_nn(X, y, cv, groups=train.installation_id, model_count=2, OneTestPerGroup=False,\n",
    "                        nn_params=None, X_test=None):\n",
    "    \"\"\"\n",
    "    Функция оценки набора признаков (в первую очередь).\n",
    "    OneTestPerGroup = True, если хотим использовать OneTestPerGroupKFold.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    accumulated_importances = np.zeros(len(X.columns))\n",
    "    # Пробуем несколько случайных состояний\n",
    "    # for seed_delta in [17-RANDOM_SEED, 28-RANDOM_SEED, 45-RANDOM_SEED]:\n",
    "#     first_layer_size = np.max([nn_params['first_layer_size'],\n",
    "#                                    nn_params['last_layer_size']])\n",
    "#     last_layer_size = np.min([nn_params['first_layer_size'],\n",
    "#                                nn_params['last_layer_size']])\n",
    "    input_size_list = [350, 250, 200, 100, 1]#np.sort(np.random.randint(low=last_layer_size,\n",
    "                       #                 high=first_layer_size,\n",
    "                        #                size=int(nn_params['layers_amount'])))[::-1]\n",
    "    input_size_list[-1] = 1\n",
    "    dropout_list = [0.3, 0.5, 0.5, 0.5, 0.5]#np.random.rand(int(nn_params['layers_amount'])) * 0.5 + 0.3\n",
    "    preds_for_test = np.zeros(len(X_test))\n",
    "    final_coefs = np.zeros(3)\n",
    "    for seed_delta in range(model_count):\n",
    "        print('Training model with seed +', seed_delta)\n",
    "        \n",
    "        ## смена сида для PARANOIDAL_CV\n",
    "        seed_torch(RANDOM_SEED + seed_delta)\n",
    "        if OneTestPerGroup:\n",
    "            splitter = cv.split(X, groups=groups)\n",
    "        else:\n",
    "            splitter = cv.split(X, y, groups=groups)\n",
    "        \n",
    "        for train_idxs, test_idxs in splitter:\n",
    "            model = DSBModel(embedding_sizes, len(X.columns) - len(categorical_features),\n",
    "                             input_size_list=input_size_list,\n",
    "                             dropout_list=dropout_list)\n",
    "            parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "            lr = 0.02\n",
    "            wd = 1e-6#nn_params['weights_decay']\n",
    "            optimizer = optim.Adam(parameters, lr=lr, weight_decay=wd, amsgrad=True)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                             factor=0.2,\n",
    "                                                             patience=3,\n",
    "                                                             verbose=True,\n",
    "                                                             eps=1e-8)\n",
    "            train_X = X.iloc[train_idxs, :]\n",
    "            train_y = y.iloc[train_idxs, :]\n",
    "            test_X = X.iloc[test_idxs, :]\n",
    "            test_y = y.iloc[test_idxs, :]\n",
    "\n",
    "            tmp_model = ClassifierWrapperNN(model=model, \n",
    "                                            optimizer=optimizer,\n",
    "                                            scheduler=scheduler,\n",
    "                                            embedded_col_names=embedded_col_names)\n",
    "            tmp_model.fit(train_X, train_y, test_X, test_y, 100)\n",
    "            ## К сожалению, feature importances не получить(\n",
    "#             accumulated_importances += tmp_model.model.feature_importances_\n",
    "            tmp_preds = tmp_model.predict(test_X)\n",
    "            tmp_score = qwk(tmp_preds, test_y.values.reshape(-1, ))\n",
    "            print('OOF score: ', tmp_score)\n",
    "            final_coefs += tmp_model.optimized_rounder.coefficients() / (model_count * PARANOIDAL_CV_FOLD_COUNT)\n",
    "            preds_for_test += tmp_model.predict_regressor(X_test) / (model_count * PARANOIDAL_CV_FOLD_COUNT)\n",
    "\n",
    "            scores.append(tmp_score)\n",
    "            \n",
    "            \n",
    "    print(input_size_list,\n",
    "          dropout_list)\n",
    "    return np.array(scores), input_size_list, dropout_list, preds_for_test, final_coefs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_list = [350, 250, 200, 100, 1]#np.sort(np.random.randint(low=last_layer_size,\n",
    "                       #                 high=first_layer_size,\n",
    "                        #                size=int(nn_params['layers_amount'])))[::-1]\n",
    "input_size_list[-1] = 1\n",
    "dropout_list = [0.3, 0.5, 0.5, 0.5, 0.5]#np.random.rand(int(nn_params['layers_amount'])) * 0.5 + 0.3\n",
    "model = DSBModel(embedding_sizes, len(train.drop(columns=exclude_features).columns) - len(categorical_features),\n",
    "                             input_size_list=input_size_list,\n",
    "                             dropout_list=dropout_list)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "lr = 0.02\n",
    "wd = 1e-6#nn_params['weights_decay']\n",
    "optimizer = optim.RMSprop(parameters, lr=lr, weight_decay=wd)#optim.Adam(parameters, lr=lr, weight_decay=wd, amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                 factor=0.2,\n",
    "                                                 patience=3,\n",
    "                                                 verbose=True,\n",
    "                                                 eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_model = ClassifierWrapperNN(model=model, \n",
    "                                      optimizer=optimizer,\n",
    "                                      scheduler=scheduler,\n",
    "                                      embedded_col_names=embedded_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## шкалируем данные\n",
    "\n",
    "test['session_title'] = test['session_title'].map({4: 0, 8: 1, 9: 2, 10: 3, 30: 4})\n",
    "test['world'] = test['world'].map({0: 0, 1: 1, 3: 2})\n",
    "## пока что заполняем  nans - '-1'. Потом, возможно, можно это сделать более умным способом.\n",
    "test.fillna(-1, inplace=True)\n",
    "test[numerical_columns] = std_scal.transform(test[numerical_columns].values.astype(np.float64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with seed + 0\n",
      "training loss: 1.552\n",
      "valid loss 1.731\n",
      "Validation loss decreased (inf --> 1.731450).  Saving model ...\n",
      "training loss: 1.254\n",
      "valid loss 1.128\n",
      "Validation loss decreased (1.731450 --> 1.128180).  Saving model ...\n",
      "training loss: 1.170\n",
      "valid loss 1.053\n",
      "Validation loss decreased (1.128180 --> 1.053461).  Saving model ...\n",
      "training loss: 1.133\n",
      "valid loss 1.044\n",
      "Validation loss decreased (1.053461 --> 1.044351).  Saving model ...\n",
      "training loss: 1.092\n",
      "valid loss 1.047\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.081\n",
      "valid loss 1.023\n",
      "Validation loss decreased (1.044351 --> 1.022539).  Saving model ...\n",
      "training loss: 1.065\n",
      "valid loss 1.017\n",
      "Validation loss decreased (1.022539 --> 1.017458).  Saving model ...\n",
      "training loss: 1.061\n",
      "valid loss 1.017\n",
      "Validation loss decreased (1.017458 --> 1.017276).  Saving model ...\n",
      "training loss: 1.046\n",
      "valid loss 1.031\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.045\n",
      "valid loss 1.015\n",
      "Validation loss decreased (1.017276 --> 1.014631).  Saving model ...\n",
      "training loss: 1.038\n",
      "valid loss 1.021\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.030\n",
      "valid loss 1.012\n",
      "Validation loss decreased (1.014631 --> 1.011809).  Saving model ...\n",
      "training loss: 1.025\n",
      "valid loss 1.019\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.023\n",
      "valid loss 1.030\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.016\n",
      "valid loss 1.012\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.008\n",
      "valid loss 1.016\n",
      "Epoch    15: reducing learning rate of group 0 to 4.0000e-03.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.997\n",
      "valid loss 1.011\n",
      "Validation loss decreased (1.011809 --> 1.010706).  Saving model ...\n",
      "training loss: 0.990\n",
      "valid loss 1.014\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 0.977\n",
      "valid loss 1.019\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 0.978\n",
      "valid loss 1.020\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 0.973\n",
      "valid loss 1.016\n",
      "Epoch    20: reducing learning rate of group 0 to 8.0000e-04.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.970\n",
      "valid loss 1.017\n",
      "EarlyStopping counter: 5 out of 10\n",
      "training loss: 0.969\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 6 out of 10\n",
      "training loss: 0.971\n",
      "valid loss 1.018\n",
      "EarlyStopping counter: 7 out of 10\n",
      "training loss: 0.963\n",
      "valid loss 1.019\n",
      "Epoch    24: reducing learning rate of group 0 to 1.6000e-04.\n",
      "EarlyStopping counter: 8 out of 10\n",
      "training loss: 0.964\n",
      "valid loss 1.019\n",
      "EarlyStopping counter: 9 out of 10\n",
      "training loss: 0.965\n",
      "valid loss 1.018\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Val: -0.6502600302220239\n",
      "New best!\n",
      "Val: -0.652050174708027\n",
      "New best!\n",
      "Val: -0.6525266152400022\n",
      "New best!\n",
      "Found bounds: [1.13079998 1.62755213 2.3215207 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-7e6567382b89>:3: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"qwk\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-33-7e6567382b89> (13)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 13:\u001b[0m\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "\u001b[1m    max_rat = 3\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-33-7e6567382b89>:3: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"qwk\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 21:\u001b[0m\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "    <source elided>\n",
      "    o = 0\n",
      "\u001b[1m    for k in range(a1.shape[0]):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"qwk\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 4:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-33-7e6567382b89>\", line 4:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef qwk(a1, a2):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF score:  0.5797113882619809\n",
      "training loss: 1.482\n",
      "valid loss 1.876\n",
      "Validation loss decreased (inf --> 1.875545).  Saving model ...\n",
      "training loss: 1.232\n",
      "valid loss 1.291\n",
      "Validation loss decreased (1.875545 --> 1.290525).  Saving model ...\n",
      "training loss: 1.142\n",
      "valid loss 1.139\n",
      "Validation loss decreased (1.290525 --> 1.138627).  Saving model ...\n",
      "training loss: 1.094\n",
      "valid loss 1.110\n",
      "Validation loss decreased (1.138627 --> 1.109549).  Saving model ...\n",
      "training loss: 1.069\n",
      "valid loss 1.098\n",
      "Validation loss decreased (1.109549 --> 1.098246).  Saving model ...\n",
      "training loss: 1.050\n",
      "valid loss 1.083\n",
      "Validation loss decreased (1.098246 --> 1.083273).  Saving model ...\n",
      "training loss: 1.049\n",
      "valid loss 1.067\n",
      "Validation loss decreased (1.083273 --> 1.066678).  Saving model ...\n",
      "training loss: 1.042\n",
      "valid loss 1.061\n",
      "Validation loss decreased (1.066678 --> 1.060640).  Saving model ...\n",
      "training loss: 1.030\n",
      "valid loss 1.064\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.021\n",
      "valid loss 1.063\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.011\n",
      "valid loss 1.081\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.010\n",
      "valid loss 1.075\n",
      "Epoch    11: reducing learning rate of group 0 to 4.0000e-03.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 1.004\n",
      "valid loss 1.054\n",
      "Validation loss decreased (1.060640 --> 1.053595).  Saving model ...\n",
      "training loss: 0.995\n",
      "valid loss 1.067\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 0.987\n",
      "valid loss 1.060\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 0.983\n",
      "valid loss 1.061\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 0.983\n",
      "valid loss 1.064\n",
      "Epoch    16: reducing learning rate of group 0 to 8.0000e-04.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.975\n",
      "valid loss 1.062\n",
      "EarlyStopping counter: 5 out of 10\n",
      "training loss: 0.971\n",
      "valid loss 1.061\n",
      "EarlyStopping counter: 6 out of 10\n",
      "training loss: 0.973\n",
      "valid loss 1.062\n",
      "EarlyStopping counter: 7 out of 10\n",
      "training loss: 0.974\n",
      "valid loss 1.062\n",
      "Epoch    20: reducing learning rate of group 0 to 1.6000e-04.\n",
      "EarlyStopping counter: 8 out of 10\n",
      "training loss: 0.969\n",
      "valid loss 1.061\n",
      "EarlyStopping counter: 9 out of 10\n",
      "training loss: 0.971\n",
      "valid loss 1.062\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Val: -0.6256781235335476\n",
      "New best!\n",
      "Val: -0.6500647688349175\n",
      "New best!\n",
      "Val: -0.649561531457368\n",
      "Found bounds: [1.00735552 1.76193587 2.30151172]\n",
      "OOF score:  0.540538336135751\n",
      "training loss: 1.428\n",
      "valid loss 1.860\n",
      "Validation loss decreased (inf --> 1.859740).  Saving model ...\n",
      "training loss: 1.238\n",
      "valid loss 1.218\n",
      "Validation loss decreased (1.859740 --> 1.218139).  Saving model ...\n",
      "training loss: 1.163\n",
      "valid loss 1.057\n",
      "Validation loss decreased (1.218139 --> 1.057165).  Saving model ...\n",
      "training loss: 1.110\n",
      "valid loss 1.054\n",
      "Validation loss decreased (1.057165 --> 1.054483).  Saving model ...\n",
      "training loss: 1.087\n",
      "valid loss 1.036\n",
      "Validation loss decreased (1.054483 --> 1.036224).  Saving model ...\n",
      "training loss: 1.078\n",
      "valid loss 1.043\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.073\n",
      "valid loss 1.058\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.063\n",
      "valid loss 1.017\n",
      "Validation loss decreased (1.036224 --> 1.016825).  Saving model ...\n",
      "training loss: 1.038\n",
      "valid loss 1.019\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.034\n",
      "valid loss 1.017\n",
      "Validation loss decreased (1.016825 --> 1.016529).  Saving model ...\n",
      "training loss: 1.035\n",
      "valid loss 1.019\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.024\n",
      "valid loss 1.130\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.021\n",
      "valid loss 1.005\n",
      "Validation loss decreased (1.016529 --> 1.004957).  Saving model ...\n",
      "training loss: 1.012\n",
      "valid loss 1.029\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.015\n",
      "valid loss 1.034\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.006\n",
      "valid loss 1.028\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 0.997\n",
      "valid loss 1.030\n",
      "Epoch    16: reducing learning rate of group 0 to 4.0000e-03.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.991\n",
      "valid loss 1.015\n",
      "EarlyStopping counter: 5 out of 10\n",
      "training loss: 0.982\n",
      "valid loss 1.019\n",
      "EarlyStopping counter: 6 out of 10\n",
      "training loss: 0.976\n",
      "valid loss 1.015\n",
      "EarlyStopping counter: 7 out of 10\n",
      "training loss: 0.972\n",
      "valid loss 1.018\n",
      "Epoch    20: reducing learning rate of group 0 to 8.0000e-04.\n",
      "EarlyStopping counter: 8 out of 10\n",
      "training loss: 0.966\n",
      "valid loss 1.018\n",
      "EarlyStopping counter: 9 out of 10\n",
      "training loss: 0.966\n",
      "valid loss 1.018\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Val: -0.6180070539228043\n",
      "New best!\n",
      "Val: -0.6326347720448884\n",
      "New best!\n",
      "Val: -0.6328110795079189\n",
      "New best!\n",
      "Found bounds: [1.18931657 1.70219361 2.28368248]\n",
      "OOF score:  0.5819361993085966\n",
      "Training model with seed + 1\n",
      "training loss: 1.518\n",
      "valid loss 1.789\n",
      "Validation loss decreased (inf --> 1.788870).  Saving model ...\n",
      "training loss: 1.264\n",
      "valid loss 1.142\n",
      "Validation loss decreased (1.788870 --> 1.141840).  Saving model ...\n",
      "training loss: 1.169\n",
      "valid loss 1.070\n",
      "Validation loss decreased (1.141840 --> 1.070118).  Saving model ...\n",
      "training loss: 1.128\n",
      "valid loss 1.090\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.104\n",
      "valid loss 1.024\n",
      "Validation loss decreased (1.070118 --> 1.024165).  Saving model ...\n",
      "training loss: 1.076\n",
      "valid loss 1.043\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.062\n",
      "valid loss 1.022\n",
      "Validation loss decreased (1.024165 --> 1.022333).  Saving model ...\n",
      "training loss: 1.062\n",
      "valid loss 1.019\n",
      "Validation loss decreased (1.022333 --> 1.018516).  Saving model ...\n",
      "training loss: 1.043\n",
      "valid loss 1.034\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.041\n",
      "valid loss 1.026\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.044\n",
      "valid loss 1.090\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.037\n",
      "valid loss 1.013\n",
      "Validation loss decreased (1.018516 --> 1.013127).  Saving model ...\n",
      "training loss: 1.020\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.019\n",
      "valid loss 1.024\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.017\n",
      "valid loss 1.020\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.008\n",
      "valid loss 1.033\n",
      "Epoch    15: reducing learning rate of group 0 to 4.0000e-03.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 1.000\n",
      "valid loss 1.010\n",
      "Validation loss decreased (1.013127 --> 1.010370).  Saving model ...\n",
      "training loss: 0.995\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 0.983\n",
      "valid loss 1.010\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 0.978\n",
      "valid loss 1.018\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 0.982\n",
      "valid loss 1.013\n",
      "Epoch    20: reducing learning rate of group 0 to 8.0000e-04.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.970\n",
      "valid loss 1.014\n",
      "EarlyStopping counter: 5 out of 10\n",
      "training loss: 0.967\n",
      "valid loss 1.015\n",
      "EarlyStopping counter: 6 out of 10\n",
      "training loss: 0.975\n",
      "valid loss 1.015\n",
      "EarlyStopping counter: 7 out of 10\n",
      "training loss: 0.969\n",
      "valid loss 1.016\n",
      "Epoch    24: reducing learning rate of group 0 to 1.6000e-04.\n",
      "EarlyStopping counter: 8 out of 10\n",
      "training loss: 0.966\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 9 out of 10\n",
      "training loss: 0.966\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Val: -0.6553319660309376\n",
      "New best!\n",
      "Val: -0.655969144918888\n",
      "New best!\n",
      "Val: -0.6560839952165594\n",
      "New best!\n",
      "Found bounds: [1.18901848 1.59889839 2.29554627]\n",
      "OOF score:  0.5732049246258442\n",
      "training loss: 1.533\n",
      "valid loss 1.824\n",
      "Validation loss decreased (inf --> 1.823660).  Saving model ...\n",
      "training loss: 1.248\n",
      "valid loss 1.455\n",
      "Validation loss decreased (1.823660 --> 1.454965).  Saving model ...\n",
      "training loss: 1.136\n",
      "valid loss 1.130\n",
      "Validation loss decreased (1.454965 --> 1.129761).  Saving model ...\n",
      "training loss: 1.094\n",
      "valid loss 1.081\n",
      "Validation loss decreased (1.129761 --> 1.080816).  Saving model ...\n",
      "training loss: 1.066\n",
      "valid loss 1.063\n",
      "Validation loss decreased (1.080816 --> 1.062667).  Saving model ...\n",
      "training loss: 1.057\n",
      "valid loss 1.065\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.051\n",
      "valid loss 1.073\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.042\n",
      "valid loss 1.076\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.029\n",
      "valid loss 1.065\n",
      "Epoch     8: reducing learning rate of group 0 to 4.0000e-03.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 1.023\n",
      "valid loss 1.059\n",
      "Validation loss decreased (1.062667 --> 1.059042).  Saving model ...\n",
      "training loss: 1.014\n",
      "valid loss 1.059\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.009\n",
      "valid loss 1.061\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.011\n",
      "valid loss 1.056\n",
      "Validation loss decreased (1.059042 --> 1.056132).  Saving model ...\n",
      "training loss: 1.013\n",
      "valid loss 1.061\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.008\n",
      "valid loss 1.056\n",
      "Validation loss decreased (1.056132 --> 1.055771).  Saving model ...\n",
      "training loss: 1.001\n",
      "valid loss 1.058\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 0.999\n",
      "valid loss 1.056\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 0.998\n",
      "valid loss 1.060\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 0.994\n",
      "valid loss 1.058\n",
      "Epoch    18: reducing learning rate of group 0 to 8.0000e-04.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.999\n",
      "valid loss 1.056\n",
      "EarlyStopping counter: 5 out of 10\n",
      "training loss: 0.995\n",
      "valid loss 1.058\n",
      "EarlyStopping counter: 6 out of 10\n",
      "training loss: 0.988\n",
      "valid loss 1.058\n",
      "EarlyStopping counter: 7 out of 10\n",
      "training loss: 0.992\n",
      "valid loss 1.057\n",
      "Epoch    22: reducing learning rate of group 0 to 1.6000e-04.\n",
      "EarlyStopping counter: 8 out of 10\n",
      "training loss: 0.994\n",
      "valid loss 1.057\n",
      "EarlyStopping counter: 9 out of 10\n",
      "training loss: 0.992\n",
      "valid loss 1.057\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Val: -0.6070478310606292\n",
      "New best!\n",
      "Val: -0.6369746689142906\n",
      "New best!\n",
      "Val: -0.6357940039566893\n",
      "Found bounds: [1.06526998 1.59989632 2.24565229]\n",
      "OOF score:  0.5287999556515595\n",
      "training loss: 1.509\n",
      "valid loss 1.940\n",
      "Validation loss decreased (inf --> 1.939709).  Saving model ...\n",
      "training loss: 1.232\n",
      "valid loss 1.271\n",
      "Validation loss decreased (1.939709 --> 1.271029).  Saving model ...\n",
      "training loss: 1.154\n",
      "valid loss 1.039\n",
      "Validation loss decreased (1.271029 --> 1.038909).  Saving model ...\n",
      "training loss: 1.110\n",
      "valid loss 1.032\n",
      "Validation loss decreased (1.038909 --> 1.032385).  Saving model ...\n",
      "training loss: 1.097\n",
      "valid loss 1.028\n",
      "Validation loss decreased (1.032385 --> 1.027767).  Saving model ...\n",
      "training loss: 1.070\n",
      "valid loss 1.021\n",
      "Validation loss decreased (1.027767 --> 1.020511).  Saving model ...\n",
      "training loss: 1.057\n",
      "valid loss 1.026\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.052\n",
      "valid loss 1.021\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.045\n",
      "valid loss 1.012\n",
      "Validation loss decreased (1.020511 --> 1.011685).  Saving model ...\n",
      "training loss: 1.037\n",
      "valid loss 1.013\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.032\n",
      "valid loss 1.017\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.027\n",
      "valid loss 1.017\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.020\n",
      "valid loss 1.010\n",
      "Validation loss decreased (1.011685 --> 1.009984).  Saving model ...\n",
      "training loss: 1.027\n",
      "valid loss 1.028\n",
      "EarlyStopping counter: 1 out of 10\n",
      "training loss: 1.011\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 2 out of 10\n",
      "training loss: 1.014\n",
      "valid loss 1.022\n",
      "EarlyStopping counter: 3 out of 10\n",
      "training loss: 1.007\n",
      "valid loss 1.037\n",
      "Epoch    16: reducing learning rate of group 0 to 4.0000e-03.\n",
      "EarlyStopping counter: 4 out of 10\n",
      "training loss: 0.997\n",
      "valid loss 1.013\n",
      "EarlyStopping counter: 5 out of 10\n",
      "training loss: 0.987\n",
      "valid loss 1.014\n",
      "EarlyStopping counter: 6 out of 10\n",
      "training loss: 0.973\n",
      "valid loss 1.015\n",
      "EarlyStopping counter: 7 out of 10\n",
      "training loss: 0.971\n",
      "valid loss 1.034\n",
      "Epoch    20: reducing learning rate of group 0 to 8.0000e-04.\n",
      "EarlyStopping counter: 8 out of 10\n",
      "training loss: 0.962\n",
      "valid loss 1.016\n",
      "EarlyStopping counter: 9 out of 10\n",
      "training loss: 0.965\n",
      "valid loss 1.018\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Val: -0.5881608077828682\n",
      "New best!\n",
      "Val: -0.6284076493456424\n",
      "New best!\n",
      "Val: -0.6292324723308169\n",
      "New best!\n",
      "Found bounds: [1.20990448 1.69263669 2.19422877]\n",
      "OOF score:  0.5774116614896198\n",
      "[350, 250, 200, 100, 1] [0.3, 0.5, 0.5, 0.5, 0.5]\n",
      "CPU times: user 10min 9s, sys: 1.99 s, total: 10min 11s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pcv_splitter = GroupKFold(PARANOIDAL_CV_FOLD_COUNT)\n",
    "cv_score, importances, _, test_preds, final_coefs = ensemble_nn(X, y, pcv_splitter, model_count=PARANOIDAL_CV_SEED_COUNT,\n",
    "                                      X_test=test.drop(columns=exclude_features))\n",
    "cv_score = np.sort(cv_score)[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.cut(test_preds, [-np.inf] + list(np.sort(final_coefs)) + [np.inf], labels = [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Важно! При использовании псевдолейблинга важности признаков не стоит доверять на 100%.\n",
    "\n",
    "# if MULTISEED_BLEND:\n",
    "#     # TODO: Сейчас берутся важности признаков просто из первого. Можно усреднять...\n",
    "#     get_importances = lambda wrapper: wrapper.model.estimators_[0].feature_importances_ \n",
    "# else:\n",
    "#     get_importances = lambda wrapper: wrapper.model.feature_importances_\n",
    "\n",
    "# features = pd.DataFrame({'feature': X.columns, 'importance': get_importances(rgr_model)})\n",
    "# features.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "# features.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## это нужно для того, чтобы эмбеддинге pytorch правильно построились\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>installation_id</th>\n",
       "      <th>accuracy_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00abaee7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01242218</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>017c5718</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01a44906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01bc6cb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  installation_id  accuracy_group\n",
       "0        00abaee7               3\n",
       "1        01242218               3\n",
       "2        017c5718               2\n",
       "3        01a44906               3\n",
       "4        01bc6cb6               2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
    "submission.accuracy_group = y_pred.astype(int)\n",
    "submission.to_csv('submission.csv', index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PSEUDOLABELING:\n",
    "    plt.title('Распределение тестовых объектов по псевдоклассам')\n",
    "    plt.hist(pseudolabels)\n",
    "    plt.xticks([0, 1, 2, 3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFltJREFUeJzt3HuUZWWd3vHvA81NISDQGmgaWke8Z0QWIvGSsIDJcNEBV9CILkSE4CQYMTo6eMmI4w1nqUxcY8ZBQfASlBnNQMTMBBV0OQYUFFFsHVsG7RaERmguo6jAL3/st/RwqOo61V3V1f3y/ax1Vu39vnu/+91n7/2ct/Y+VakqJEn92mqxOyBJWlgGvSR1zqCXpM4Z9JLUOYNekjpn0EubkSRbJfG61LzyhJIWWZJ/n+TLSdYAdwAHLXaf1Jcli92B+ZbkBuBRwH3APwOfA/5LVd29mP2SppPkOOBM4MXAV8s/bNEC6HVE/7yq2hHYH3g68OZF7o80k3cCL6yqfzDktVB6DXoAquonwP8BngKQ5MQkK5PcleT6JK8YXT7J0UmuSXJnkh8mObyVX57kniR3t9cv2m8OU+vdkOQNSb6b5PYkH0my/Uj9c1u765J8Ncnvjm3340l+NdL2mpG67ZK8J8mPk9yc5INJdhipX5GkRvp2X5KTW91WSU5v+/KzJBcm2XVsvSVj/TijTR881o8XtuVPHil7eXs/b0/y90n2me44JPnWyL7dP9LXN7b6JyS5NMltSb6f5IUj6+6Q5L1JfpTkjiRfaWWztfnMJF9v63w9yTNH2hw9nrckecdI3XlJ3j7NPpyc5PKRtm9NsrzNP7Ud2yfMsP/T9iXJI4FHAqe29n6U5M3tuO09sk+V5LEztL1nkk8nWZvkn5K8aqTujCQfb9PbJ/lSkndPeEw26P2bpn+zXTtPbMusS3Jdkj+Ypa2pc/t3kqxO8ryR+oPH9uX+JIe1uqOSfDPDtb166jwfWffZGa7Nda3+Za182vOv1f11kp+28i8nefJIe+e14/a0kbI/a2WHzbSPC6aqunoBNwCHtenlwHXA29r8UcDvAAH+LfBzYP9WdyDD/dHfY/gAXAY8odVdDpw8so3DgBvGtvmdtr1dgX8A3t7q9gduAZ4BbA2c0JbfbmT9TwBvadMHA2tG6v4cuLi1uxPwv4F3jdQ/Bihg6/G+Aq8GrgD2ArYD/gq4oNWtaOstGWnr48AZ4/0AtgG+D9w40vYxwCrgiQy3AN/McOthfcfmAfvWyh4OrAZObO3sD9wKPLnVf6Dt07L2/j1z7L2brs1dgduB41ubx7X53aZ5jx4H3AM8pc2fN3Xsxto8Gbh8ZP4dwBeBHYBrgVfOsM8z9mXkGFzUju0K4B+Bk8baKOCx07S9FXA18CfAtu1cuB74/VZ/RjumSxjOoQ9NeEw2+P2bpv3fLDt+7bTzahXwxtb/Q4C7gMevry3gX7b1XjpWfwiweoYsOBj4V+09+13gZuCYVrd32+5xrU+7AfvNdv4BL2/HbTuG6/SakW2fB6wEPjCyryuBn071aVO+eh3R/22SdcBXgC8x/HpMVV1SVT+swZeA/ws8p61zEnBuVV1aVfdX1U+q6ntz2OZfVNXqqrqNIQSOa+X/Efirqrqyqu6rqvOBX/LAB247AL8abzBJ2vr/tapuq6q72r68aGSxbYH7q+q+afr0CuBNVbWmqn7JcOEfm5FR/IReAVzJEEKjZe+qqpVVdW/r136ZYVS/Hs9luPA/UlX3VtU3gE+3fm7FcDGd1o7HfVX11bYv63MU8IOq+lhr8wLge8Dzpll2CcPznDvm2O8zgJ2BrzF8AH5gI/ryhqq6q6puAN7LELCTeDqwtKr+tKp+VVXXAx/igedHgHOAHYE/nLDdTfH+wXAN7Aic2fr/ReCz/Pbamc4uDNftJ6rqo2N12zLNdQRQVZdX1bfbtX0tcAHDYA/gJcDnq+qCqvp1Vf2sqq6Z7fyrqnPbcZu6tp6aZOeRzV4MHNZ+A3ge8HmGD8VNrtegP6aqdqmqfarqP1fVLwCSHJHkigy3CNYBRwK7t3WWAz/ciG2uHpn+EbBnm94HeG37lXBd2+7ykXoYRihrp2lzKfAw4OqRdf+ulU+ZGn1NZx/gf42su5LhonzUyDK3jtS/cLyBJDsBrwf+2zRt//eRdW9jCJVlM/RlJvsAzxh7f17C8J7sDmzP3I/LngzHYNSPxvr2/rat6xg+4EeP3x+1vtyc5DNJdhvfQFX9mmHU9hTgvdWGbXPsyy9H5mfq55RvtD5dn+S1rWwfYM+x9+6NPPD4Pp/ht64n88DzZn029v2b1J4MI/D717OdcX8K3A0cmgd/DXXGayHJM5Jc1m5x3cHwoTfbtT/j+Zdk6yRnZrgteifDbw9T60z5NcNv4McyDCQ/vJ79WlC9Bv2DJNmOYaT4HuBRVbULwzdy0hZZzXBbZ0MtH5nem2GUN9XuO9oHz9TrYW2URJJtGMLiW9O0eSvwC4bbGFPr7lzDg+Ypj+OBI+1Rq4Ejxra9fQ3PLqbsPlUHXDhNG68DLqyq8Qt/NfCKsbZ3qKqvztCXmawGvjTWzo5V9Z/a/t/D3I/LjQwhOGpvYHS/X9X2eVfg2Rm+/TLlPa3uMQwftK8b30CSZcBbgI8A723n11z7cjND2O8zTd24/VufjgLeluF5wGrgn8beu52q6siR9a5nuKVxDvA/ZujjXPo8ZX3v36RuBJaPBfZM+z/lQuDZbfqVY3Xruxb+J8MIe3lV7Qx8kNmv/fWdfy8Gjma4FbUzw203Rtqc8mGGgdJuVTXdNb5JPGSCnuHXuu0YRs73JjkC+Hcj9ecAJyY5NMPDsGWZ4eHaDE5NsleGh51vBD7Vyj8E/GEbUSTJw9uDoZ1a/YkM9+2uGm+wjXQ+BJyV4cEdrV+/36aXA6cBfztDnz4IvGPqdkqSpUmOnsM+7dT6N93Dtg8Cb5h6AJVk5yQvmEPbUz4LPC7J8Um2aa+nJ3li2/9zgfdleOi4dZJ/vZ5QnfK51uaLkyxJ8h+AJ7VtjbuP4R74dKPdexie4zzgOmm31M5jOGdOAm4C3jbXvrT9u5DhGO3UjtNrGO6rz+Q+hjDZiuG20Z1J/rg9NNw6yVOSPH1k+Wtq+GrxW4EntO3PZr7ev9lcyfAV6Ne3434wwy2OT65nna+09+3lwJ8keQxAkie1spmuhZ2A26rqniQHMgT1lE8w3GJ5Ydvf3ZLsN8v5txPDh/TPGAYD75xuo+32798B75r97VhAM92831JfjDyAmabuVIZR1DrgYwwn1NtH6p/P8GDtLoaHPVMPtS5n9oexbwC+29o+H3jYSP3hwNdb3U3AXzOcKC9huEh+zfDr6N0MI/j7gQ+2dbdnOImuB+5kuP3yqlb3XeAsYJuRbf2mrwxh8BqGB6l3MfwK+s5Wt4LZH8YW8Lrp2m7zxwPfbv1azfAr/PqOzcGMPfhr5Y8HLmH4EP4Zw0POqYdhOzA86PoJw33gLwM7TNDmsxkeVN7Rfj57bD/uae/3bQyjvYe3uvPa/qxp2/wcwzdjfvMwluHD9Vpg2za/Z+v7c2bY7/X15REMQXMr8GOGB6tbja1fDIF4N8Mo+E0jdXsy3G/+KcNtiyv47QPIM4CPjyz7DIbzb/eFev+maWf8nBm/dp7M8BztDobz+fnrOX/G2/qjdq48vB2vN40tf8PIe3Esw22huxg+sP5i7L15DsMHz9S5fML6zj+GZwsXtfZ+BLyUkYfmzPxQ/zd92pSvtI1rI2T4utjJVfX5Oa73MmBFVZ0xVr4Xw0nysnnqoqSHsIfSrZvN0T8zjCDG3cswUpKkjeaIfh5s6IhekjYFg16SOuetG0nq3Gbx3yt33333WrFixWJ3Q5K2KFdfffWtVTXrV1s3i6BfsWIFV131oK+RS5LWI8n4HzJOy1s3ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUuc3iL2MlaTGtOP2SRdv2DWceteDbcEQvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOTRz0SbZO8s0kn23zj05yZZIfJPlUkm1b+XZtflWrX7EwXZckTWIuI/rTgJUj8+8GzqqqfYHbgZNa+UnA7VX1WOCstpwkaZFMFPRJ9gKOAj7c5gMcAvxNW+R84Jg2fXSbp9Uf2paXJC2CSUf0fw68Hri/ze8GrKuqe9v8GmBZm14GrAZo9Xe05R8gySlJrkpy1dq1azew+5Kk2cwa9EmeC9xSVVePFk+zaE1Q99uCqrOr6oCqOmDp0qUTdVaSNHdLJljmWcAfJDkS2B74Fwwj/F2SLGmj9r2AG9vya4DlwJokS4CdgdvmveeSpInMOqKvqjdU1V5VtQJ4EfDFqnoJcBlwbFvsBOCiNn1xm6fVf7GqHjSilyRtGhvzPfo/Bl6TZBXDPfhzWvk5wG6t/DXA6RvXRUnSxpjk1s1vVNXlwOVt+nrgwGmWuQd4wTz0TZI0D/zLWEnqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM7NGvRJtk/ytSTfSnJdkre28kcnuTLJD5J8Ksm2rXy7Nr+q1a9Y2F2QJK3PJCP6XwKHVNVTgf2Aw5McBLwbOKuq9gVuB05qy58E3F5VjwXOastJkhbJrEFfg7vb7DbtVcAhwN+08vOBY9r00W2eVn9oksxbjyVJczLRPfokWye5BrgFuBT4IbCuqu5ti6wBlrXpZcBqgFZ/B7DbNG2ekuSqJFetXbt24/ZCkjSjiYK+qu6rqv2AvYADgSdOt1j7Od3ovR5UUHV2VR1QVQcsXbp00v5KkuZoTt+6qap1wOXAQcAuSZa0qr2AG9v0GmA5QKvfGbhtPjorSZq7Sb51szTJLm16B+AwYCVwGXBsW+wE4KI2fXGbp9V/saoeNKKXJG0aS2ZfhD2A85NszfDBcGFVfTbJd4FPJnk78E3gnLb8OcDHkqxiGMm/aAH6LUma0KxBX1XXAk+bpvx6hvv14+X3AC+Yl95JkjaafxkrSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdm+RfIEhaBCtOv2RRtnvDmUctyna1cBzRS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdmzXokyxPclmSlUmuS3JaK981yaVJftB+PqKVJ8n7k6xKcm2S/Rd6JyRJM5tkRH8v8NqqeiJwEHBqkicBpwNfqKp9gS+0eYAjgH3b6xTgL+e915Kkic0a9FV1U1V9o03fBawElgFHA+e3xc4HjmnTRwMfrcEVwC5J9pj3nkuSJjKne/RJVgBPA64EHlVVN8HwYQA8si22DFg9stqaVjbe1ilJrkpy1dq1a+fec0nSRCYO+iQ7Ap8GXl1Vd65v0WnK6kEFVWdX1QFVdcDSpUsn7YYkaY4mCvok2zCE/Ceq6jOt+OapWzLt5y2tfA2wfGT1vYAb56e7kqS5muRbNwHOAVZW1ftGqi4GTmjTJwAXjZS/tH375iDgjqlbPJKkTW/JBMs8Czge+HaSa1rZG4EzgQuTnAT8GHhBq/sccCSwCvg5cOK89liSNCezBn1VfYXp77sDHDrN8gWcupH9kiTNE/8yVpI6Z9BLUucMeknq3CQPYzdrK06/ZNG2fcOZRy3atiVpUo7oJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ2bNeiTnJvkliTfGSnbNcmlSX7Qfj6ilSfJ+5OsSnJtkv0XsvOSpNlNMqI/Dzh8rOx04AtVtS/whTYPcASwb3udAvzl/HRTkrShZg36qvoycNtY8dHA+W36fOCYkfKP1uAKYJcke8xXZyVJc7dkA9d7VFXdBFBVNyV5ZCtfBqweWW5NK7tpvIEkpzCM+tl77703sBva1FacfsmibPeGM49alO1KPZjvh7GZpqymW7Cqzq6qA6rqgKVLl85zNyRJUzY06G+euiXTft7SytcAy0eW2wu4ccO7J0naWBsa9BcDJ7TpE4CLRspf2r59cxBwx9QtHknS4pj1Hn2SC4CDgd2TrAHeApwJXJjkJODHwAva4p8DjgRWAT8HTlyAPkuS5mDWoK+q42aoOnSaZQs4dWM7JUmaP/5lrCR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucWJOiTHJ7k+0lWJTl9IbYhSZrMvAd9kq2BDwBHAE8CjkvypPnejiRpMgsxoj8QWFVV11fVr4BPAkcvwHYkSRNIVc1vg8mxwOFVdXKbPx54RlW9cmy5U4BT2uzjge9v4CZ3B27dwHW1cDwumx+PyeZpY47LPlW1dLaFlmxg4+uTacoe9GlSVWcDZ2/0xpKrquqAjW1H88vjsvnxmGyeNsVxWYhbN2uA5SPzewE3LsB2JEkTWIig/zqwb5JHJ9kWeBFw8QJsR5I0gXm/dVNV9yZ5JfD3wNbAuVV13XxvZ8RG3/7RgvC4bH48JpunBT8u8/4wVpK0efEvYyWpcwa9JHVuiw56/9XC5ifJuUluSfKdxe6LBkmWJ7ksycok1yU5bbH79FCXZPskX0vyrXZM3rqg29tS79G3f7Xwj8DvMXyl8+vAcVX13UXt2ENckn8D3A18tKqestj9ESTZA9ijqr6RZCfgauAYr5XFkyTAw6vq7iTbAF8BTquqKxZie1vyiN5/tbAZqqovA7ctdj/0W1V1U1V9o03fBawEli1urx7aanB3m92mvRZs1L0lB/0yYPXI/Bo8eaX1SrICeBpw5eL2REm2TnINcAtwaVUt2DHZkoN+on+1IGmQZEfg08Crq+rOxe7PQ11V3VdV+zH894ADkyzYrc4tOej9VwvShNp94E8Dn6iqzyx2f/RbVbUOuBw4fKG2sSUHvf9qQZpAe/B3DrCyqt632P0RJFmaZJc2vQNwGPC9hdreFhv0VXUvMPWvFlYCFy7wv1rQBJJcAPw/4PFJ1iQ5abH7JJ4FHA8ckuSa9jpysTv1ELcHcFmSaxkGrZdW1WcXamNb7NcrJUmT2WJH9JKkyRj0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXP/HwWl9U+XPzXuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Распределение тестовых объектов по классам')\n",
    "plt.hist(submission['accuracy_group'])\n",
    "plt.xticks([0, 1, 2, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ADVERSARIAL_VALIDATION and not IS_SUBMITTING:\n",
    "#     from sklearn.metrics import roc_auc_score\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "\n",
    "#     av_clf = CatBoostClassifier(random_state = RANDOM_SEED,\n",
    "#                                 loss_function = 'Logloss',\n",
    "#                                 cat_features=categorical_features,\n",
    "#                                 custom_metric='AUC',\n",
    "#                                 eval_metric='AUC',\n",
    "#                                 verbose=100,\n",
    "#                                 metric_period=100,\n",
    "#                                 num_trees=400)\n",
    "\n",
    "#     X_train = train.drop(columns=exclude_features)\n",
    "#     y_train = np.zeros(X_train.shape[0])\n",
    "#     X_test  = test.drop(columns=exclude_features)\n",
    "#     y_test  = np.ones(X_test.shape[0])\n",
    "\n",
    "#     X = pd.concat([X_train, X_test], axis=0)\n",
    "#     y = np.concatenate([y_train, y_test])\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "#     av_clf.fit(X_train, y_train)           \n",
    "\n",
    "#     av_roc_auc = roc_auc_score(y_test, av_clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "#     if av_roc_auc < 0.6:\n",
    "#         print(f'AV ROC AUC {av_roc_auc:.3f}')\n",
    "#     else:\n",
    "#         print(f'AV ROC AUC {av_roc_auc:.3f} :::: WARNING!!! ')\n",
    "#         print('The most suspicious features are:')\n",
    "#         features = pd.DataFrame({'feature': X.columns, 'importance': av_clf.feature_importances_})\n",
    "#         features.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "#         for i in range(10):\n",
    "#             print(f'  {features.feature.iloc[i]}: {features.importance.iloc[i]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "291b6a67261e4044a5155b8dbb3d45fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "2e9329c5e6af4928ab5a6ca65b0955e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f600c1c4a374a8ea944506db41a9666": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3103bd2a81964f9face00bab7b1937b3",
       "max": 588,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_291b6a67261e4044a5155b8dbb3d45fd",
       "value": 588
      }
     },
     "3103bd2a81964f9face00bab7b1937b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3be2eec62d824c77bf7b4d47751ba539": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "431ffbabd05741a9931bb5617d167205": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "61cde458dbd3490b907ceaa33c400f43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "72693b9f4a9447a09d26d623e217e8fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "738b79567dc84da385aa7fcd4428720e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f3a4799ed034e7da3dc2156a35f940a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a9728461ad6946e3a46b7026c959ce35",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9e6c8405a85f41f8b03380ee3199c88d",
       "value": 1000
      }
     },
     "8c21da37322547768d0af3f620bfaba8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93c4ff929aa14dd4a25a2fde0fa53b29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fb8283fd18634c4e9feb2b00b1b5e7e2",
        "IPY_MODEL_a105a17a45524920822323e75cd26062"
       ],
       "layout": "IPY_MODEL_3be2eec62d824c77bf7b4d47751ba539"
      }
     },
     "945035988e1a4452b02f90cc39ade261": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7f3a4799ed034e7da3dc2156a35f940a",
        "IPY_MODEL_dbab0297796a4d49b777601e2e5cc51c"
       ],
       "layout": "IPY_MODEL_c14ac5c406f644778435c3b8c13bf6cd"
      }
     },
     "9e6c8405a85f41f8b03380ee3199c88d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a105a17a45524920822323e75cd26062": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e6b4ec3945c2401c8a3e48b4561025b7",
       "placeholder": "​",
       "style": "IPY_MODEL_a30ff1157772468c8da091bc37e0ee39",
       "value": " 3614/3614 [17:48&lt;00:00,  3.38it/s]"
      }
     },
     "a30ff1157772468c8da091bc37e0ee39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a33ee611c3de401f9fc0d4d4e24686bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a9728461ad6946e3a46b7026c959ce35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c14ac5c406f644778435c3b8c13bf6cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c15341bf18b94d30a46e9ba8180ec8fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2f600c1c4a374a8ea944506db41a9666",
        "IPY_MODEL_d5cc4c6f1caa42b1a9068c0907376236"
       ],
       "layout": "IPY_MODEL_738b79567dc84da385aa7fcd4428720e"
      }
     },
     "d5cc4c6f1caa42b1a9068c0907376236": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72693b9f4a9447a09d26d623e217e8fb",
       "placeholder": "​",
       "style": "IPY_MODEL_431ffbabd05741a9931bb5617d167205",
       "value": " 588/588 [05:13&lt;00:00,  1.87it/s]"
      }
     },
     "dbab0297796a4d49b777601e2e5cc51c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2e9329c5e6af4928ab5a6ca65b0955e0",
       "placeholder": "​",
       "style": "IPY_MODEL_61cde458dbd3490b907ceaa33c400f43",
       "value": " 1000/1000 [02:08&lt;00:00,  7.78it/s]"
      }
     },
     "e6b4ec3945c2401c8a3e48b4561025b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb8283fd18634c4e9feb2b00b1b5e7e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8c21da37322547768d0af3f620bfaba8",
       "max": 3614,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a33ee611c3de401f9fc0d4d4e24686bc",
       "value": 3614
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
